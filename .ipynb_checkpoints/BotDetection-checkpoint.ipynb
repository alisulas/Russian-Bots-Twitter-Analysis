{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMOD 5410 - Big Data\n",
    "## Project: Bot Detection\n",
    "### By: Matt Emmons (0221920)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Write stuff about this project. 3k+ words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.utils\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(101)\n",
    "rand_seed = 101\n",
    "\n",
    "n_rows = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.78 s, sys: 1.37 s, total: 9.15 s\n",
      "Wall time: 9.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ROOT = \"./csv\"\n",
    "df_nbc = pd.read_csv(\n",
    "    ROOT + \"/tweets.csv\", \n",
    "    encoding='utf-8', \n",
    "    low_memory=False, \n",
    "    parse_dates=False\n",
    ")\n",
    "\n",
    "df_scraped = pd.read_csv(\n",
    "    ROOT + \"/scraped_tweets.csv\", \n",
    "    nrows=n_rows, \n",
    "    encoding='utf-8', \n",
    "    low_memory=False, \n",
    "    parse_dates=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Let's remove unneeded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "delcols = [\n",
    "    'created_at',\n",
    "    'created_str',\n",
    "    'expanded_urls',\n",
    "    'in_reply_to_status_id',\n",
    "    'source'\n",
    "]\n",
    "for col in delcols:\n",
    "    del df_nbc[col]\n",
    "    del df_scraped[col]\n",
    "    \n",
    "del df_nbc['posted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to ensure all columns have consistent datatypes between the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nbc[['user_id', 'retweet_count', 'favorite_count', 'tweet_id', 'retweeted_status_id']] = df_nbc[['user_id', 'retweet_count', 'favorite_count', 'tweet_id', 'retweeted_status_id']].fillna(0).astype(int)\n",
    "df_nbc[['user_key', 'text']] = df_nbc[['user_key', 'text']].astype('str')\n",
    "df_nbc[['retweeted']] = df_nbc[['retweeted']].astype('bool')\n",
    "df_scraped[['retweeted_status_id']] = df_scraped[['retweeted_status_id']].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id                 int64\n",
      "user_key               object\n",
      "retweet_count           int64\n",
      "retweeted                bool\n",
      "favorite_count          int64\n",
      "text                   object\n",
      "tweet_id                int64\n",
      "hashtags               object\n",
      "mentions               object\n",
      "retweeted_status_id     int64\n",
      "dtype: object\n",
      "user_id                 int64\n",
      "user_key               object\n",
      "retweet_count           int64\n",
      "retweeted                bool\n",
      "favorite_count          int64\n",
      "text                   object\n",
      "tweet_id                int64\n",
      "hashtags               object\n",
      "mentions               object\n",
      "retweeted_status_id     int64\n",
      "dtype: object\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(df_nbc.dtypes)\n",
    "print(df_scraped.dtypes)\n",
    "\n",
    "print(list(df_nbc.dtypes) == list(df_scraped.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id         user_key  retweet_count  retweeted  favorite_count  \\\n",
      "0  2532611755        kathiemrr              0       True               0   \n",
      "1  2531159968   traceyhappymom              0       True               0   \n",
      "2           0    evewebster373              0       True               0   \n",
      "3  4840551713      blacktolive             18      False              17   \n",
      "4  1694026190  jacquelinisbest              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0    #ThingsDoneByMistake kissing auntie in the lips  836227891897651200   \n",
      "1  RT @mc_derpin: #TheOlderWeGet the more pessimi...  765198948239810560   \n",
      "2  RT @dmataconis: Ready To Feel Like A Failure? ...  616002306572746752   \n",
      "3    Amen! #blacklivesmatter https://t.co/wGffaOqgzl  776693302926147584   \n",
      "4  RT @NahBabyNah: Twitchy: Chuck Todd caught out...  777594647875059712   \n",
      "\n",
      "                  hashtags        mentions  retweeted_status_id  \n",
      "0  [\"ThingsDoneByMistake\"]              []                    0  \n",
      "1        [\"TheOlderWeGet\"]              []                    0  \n",
      "2                       []              []                    0  \n",
      "3     [\"Blacklivesmatter\"]              []                    0  \n",
      "4        [\"WakeUpAmerica\"]  [\"nahbabynah\"]   777591478206029824  \n",
      "\n",
      "---------------------\n",
      "\n",
      "              user_id         user_key  retweet_count  retweeted  \\\n",
      "0  961701459900342272  Jeannie22757716              0      False   \n",
      "1  961701477164105728        _p_body__              0      False   \n",
      "2  961701487507312641   6728FixerUpper              0      False   \n",
      "3  961701511289073665        LenAulett              0      False   \n",
      "4  961701912880988161       Dsquared69              0      False   \n",
      "\n",
      "   favorite_count                                               text  \\\n",
      "0               0  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...   \n",
      "1               0  b'\"We do not deserve any right \\xe2\\x80\\x94 an...   \n",
      "2               0  b'Just say no to Martha McSally, who voted for...   \n",
      "3               0  b'Pelosi Thanks Illegals for BREAKING THE LAW,...   \n",
      "4               0  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...   \n",
      "\n",
      "             tweet_id hashtags             mentions  retweeted_status_id  \n",
      "0  961701459900342272       []         ['bbusa617']   961624822953512960  \n",
      "1  961701477164105728       []      ['nprpolitics']   961390308553576448  \n",
      "2  961701487507312641       []  ['LarrySchweikart']   960538375546593280  \n",
      "3  961701511289073665       []         ['bbusa617']   961604790689165312  \n",
      "4  961701912880988161       []         ['bbusa617']   961624822953512960  \n"
     ]
    }
   ],
   "source": [
    "print(df_nbc.head())\n",
    "print(\"\\n---------------------\\n\")\n",
    "print(df_scraped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id         user_key  retweet_count  retweeted  favorite_count  \\\n",
      "0  2532611755        kathiemrr              0       True               0   \n",
      "1  2531159968   traceyhappymom              0       True               0   \n",
      "2           0    evewebster373              0       True               0   \n",
      "3  4840551713      blacktolive             18      False              17   \n",
      "4  1694026190  jacquelinisbest              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0    #ThingsDoneByMistake kissing auntie in the lips  836227891897651200   \n",
      "1  RT @mc_derpin: #TheOlderWeGet the more pessimi...  765198948239810560   \n",
      "2  RT @dmataconis: Ready To Feel Like A Failure? ...  616002306572746752   \n",
      "3    Amen! #blacklivesmatter https://t.co/wGffaOqgzl  776693302926147584   \n",
      "4  RT @NahBabyNah: Twitchy: Chuck Todd caught out...  777594647875059712   \n",
      "\n",
      "                  hashtags        mentions  retweeted_status_id  \n",
      "0  [\"ThingsDoneByMistake\"]              []                    0  \n",
      "1        [\"TheOlderWeGet\"]              []                    0  \n",
      "2                       []              []                    0  \n",
      "3     [\"Blacklivesmatter\"]              []                    0  \n",
      "4        [\"WakeUpAmerica\"]  [\"nahbabynah\"]   777591478206029824  \n",
      "\n",
      "---------------------\n",
      "\n",
      "              user_id         user_key  retweet_count  retweeted  \\\n",
      "0  961701459900342272  Jeannie22757716              0      False   \n",
      "1  961701477164105728        _p_body__              0      False   \n",
      "2  961701487507312641   6728FixerUpper              0      False   \n",
      "3  961701511289073665        LenAulett              0      False   \n",
      "4  961701912880988161       Dsquared69              0      False   \n",
      "\n",
      "   favorite_count                                               text  \\\n",
      "0               0  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...   \n",
      "1               0  b'\"We do not deserve any right \\xe2\\x80\\x94 an...   \n",
      "2               0  b'Just say no to Martha McSally, who voted for...   \n",
      "3               0  b'Pelosi Thanks Illegals for BREAKING THE LAW,...   \n",
      "4               0  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...   \n",
      "\n",
      "             tweet_id hashtags             mentions  retweeted_status_id  \n",
      "0  961701459900342272       []         ['bbusa617']   961624822953512960  \n",
      "1  961701477164105728       []      ['nprpolitics']   961390308553576448  \n",
      "2  961701487507312641       []  ['LarrySchweikart']   960538375546593280  \n",
      "3  961701511289073665       []         ['bbusa617']   961604790689165312  \n",
      "4  961701912880988161       []         ['bbusa617']   961624822953512960  \n"
     ]
    }
   ],
   "source": [
    "# df_nbc['newtext'] = df_nbc.text.str.decode(encoding = 'UTF-8')\n",
    "# df_scraped['newtext'] = df_scraped.text.str.decode(encoding = 'UTF-8')\n",
    "\n",
    "# df_nbc['newtext'] = df_nbc.text.apply( lambda x:  x.decode(encoding = \"utf-8\"))\n",
    "# df_scraped['newtext'] = df_scraped.text.apply( lambda x:  x.decode(encoding = \"utf-8\"))\n",
    "\n",
    "\n",
    "print(df_nbc.head())\n",
    "print(\"\\n---------------------\\n\")\n",
    "print(df_scraped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "\n",
    "What we need to do is this:\n",
    "\n",
    "- Attach a class feature to the nbc dataset\n",
    "- Since we do not know the class of the scraped dataset, we leave it for now\n",
    "- Create a new dataset merged between a subset of nbc and scraped datasets to be our training set\n",
    "- All rows left out of the merged subset will become the test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nbc['class'] = 1\n",
    "df_test = df_scraped.copy()\n",
    "df_test['class'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id         user_key  retweet_count  retweeted  favorite_count  \\\n",
      "0  2532611755        kathiemrr              0       True               0   \n",
      "1  2531159968   traceyhappymom              0       True               0   \n",
      "2           0    evewebster373              0       True               0   \n",
      "3  4840551713      blacktolive             18      False              17   \n",
      "4  1694026190  jacquelinisbest              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0    #ThingsDoneByMistake kissing auntie in the lips  836227891897651200   \n",
      "1  RT @mc_derpin: #TheOlderWeGet the more pessimi...  765198948239810560   \n",
      "2  RT @dmataconis: Ready To Feel Like A Failure? ...  616002306572746752   \n",
      "3    Amen! #blacklivesmatter https://t.co/wGffaOqgzl  776693302926147584   \n",
      "4  RT @NahBabyNah: Twitchy: Chuck Todd caught out...  777594647875059712   \n",
      "\n",
      "                  hashtags        mentions  retweeted_status_id  class  \n",
      "0  [\"ThingsDoneByMistake\"]              []                    0    1.0  \n",
      "1        [\"TheOlderWeGet\"]              []                    0    1.0  \n",
      "2                       []              []                    0    1.0  \n",
      "3     [\"Blacklivesmatter\"]              []                    0    1.0  \n",
      "4        [\"WakeUpAmerica\"]  [\"nahbabynah\"]   777591478206029824    1.0  \n",
      "user_id                  int64\n",
      "user_key                object\n",
      "retweet_count            int64\n",
      "retweeted                 bool\n",
      "favorite_count           int64\n",
      "text                    object\n",
      "tweet_id                 int64\n",
      "hashtags                object\n",
      "mentions                object\n",
      "retweeted_status_id      int64\n",
      "class                  float64\n",
      "dtype: object\n",
      "1203451\n"
     ]
    }
   ],
   "source": [
    "# Merge df_test and df_nbc\n",
    "# train/test/val split\n",
    "# apply classifiers\n",
    "df = pd.concat([df_nbc, df_test], ignore_index = True)\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "print(len(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', '\"', '@POTUS', '@realDonaldTrump', '\\\\', 'nRemember', 'this', 'tweet', '?', 'You', 'are', 'an', \"f'ing\", 'Hypocrite', '...', 'as', 'you', 'do', 'not', 'read', 'yours', '.', 'https://t.co/O9rjrFoQFd', '\"']\n",
      "['RT', '@EvolvedGuppy', ':', '@tlcprincess', '@bunnyhugger75', 'alert', 'the', 'perimeter', 'defense', 'system']\n",
      "['b', \"'\", '@TrueFactsStated', 'The', 'one', 'time', 'I', 'wish', 'someone', 'would', 'wear', 'a', 'MAGA', 'hat', '.', \"'\"]\n",
      "['b', '\"', '\\\\', 'xe2', '\\\\', 'x80', '\\\\', 'xbc', '\\\\', 'xef', '\\\\', 'xb8', '\\\\', 'x8f', '\\\\', 'xc2', '\\\\', 'xa0This', 'is', 'HUGE', '.', 'Brand', 'is', 'next', 'in', 'the', 'line', 'of', 'succession', 'to', 'take', 'over', 'the', 'Mueller', 'investigation', 'if', 'Trump', 'fires', 'Rod', 'Rosenstein', '.', 'The', 'chess', 'pieces', 'are', 'moving', '.', 'Good', 'money', 'that', 'this', 'has', 'everything', 'to', 'do', 'with', \"Trump's\", 'desire', 'to', 'axe', 'Rosenstein', '.', '\\\\', 'n', '\\\\', 'nBe', 'ready', ':', 'https://t.co/BCXIKPjD8C', 'https://t.co/e6ofoO4gQD', '\"']\n",
      "[\"b'hey\", 'guys', 'sorry', ',', 'just', 'getting', 'online', ',', 'was', 'watching', 'fox', 'news', 'all', 'day', '\\\\', 'n', '\\\\', 'npretty', 'crazy', 'how', 'barack', 'obama', 'just', 'broke', 'into', 'congress', 'and', 'forced', 'them', 'to', 'shut', 'down', 'the', 'government', 'at', 'gunpoint', 'for', 'the', 'second', 'time', 'this', 'week', '\\\\', 'n', '\\\\', 'nthey', 'really', 'need', 'to', 'put', 'bars', 'on', 'the', 'windows', 'this', 'is', 'getting', 'ridiculous', \"'\"]\n",
      "['Hotel', 'Growlifornia', '#DogSongs', '@midnight']\n",
      "[\"b'Yesterday\", ':\\\\', 'n', '-', 'the', 'Dow', 'was', 'down', 'over', '1,000', '\\\\', 'n', '-', 'the', 'government', 'shut', 'down', '\\\\', 'n', '-', 'the', 'Trump', 'regime', 'covered', 'for', 'a', 'man', 'who', 'beat', 'up', 'women', ',', 'saw', 'classified', 'info', ',', 'could', 'be', 'blackmailed', '\\\\', 'n', '-', 'Trump', 'is', 'still', 'reading', '\\\\', 'xe2', '\\\\', 'x80', '\\\\', 'x9clengthy', '\\\\', 'xe2', '\\\\', 'x80', '\\\\', 'x9d', '10', '-', 'page', 'Democratic', 'memo', '\\\\', 'n', '-', 'GOP', 'threw', 'more', 'tactics', 'to', 'discredit', 'FBI', 'and', 'Mueller', '.', \"'\"]\n",
      "[\"b'PLEASE\", 'TAKE', 'this', 'POLL', 'on', 'what', 'YOU', 'will', 'do', 'if', 'Trump', 'fires', 'Mueller', '.', 'If', 'there', 'is', 'a', 'large', 'enough', 'response', ',', 'it', 'will', 'BE', 'SENT', 'TO', 'NEWS', 'AGENCIES', 'AND', 'CONGRESS', 'as', 'a', '\"', 'warning', '\"', '...', 'Even', 'if', 'you', 'do', 'not', 'plan', 'on', 'doing', 'a', 'protest', ',', 'please', 'respond', '.', 'YOUR', 'VOICE', 'IS', 'IMPORTANT', '.', 'https://t.co/bMpWty6GaD', \"'\"]\n",
      "['RT', '@DailySanAntonio', ':', 'A', '&', 'M', 'regents', ',', 'in', 'board', 'shuffle', ',', 'name', 'Mendoza', 'as', 'vice', 'chair', '#local', '#news']\n",
      "[\"b'RT\", '@PreetBharara', ':', '#ReleaseTheMemo', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "class NormalizationPipeline():\n",
    "    \"\"\"\n",
    "    1. tokenize\n",
    "    2. replace_contractions\n",
    "    3. remove_non_ascii\n",
    "    4. to_lowercase\n",
    "    5. remove_punctuation\n",
    "    6. replace_numbers\n",
    "    7. remove_stopwords\n",
    "    \"\"\"\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        self.process()\n",
    "\n",
    "    def process(self):\n",
    "        self.words = self.replace_contractions()\n",
    "        self.words = nltk.word_tokenize(self.words)\n",
    "        return self.words\n",
    "\n",
    "    def replace_contractions(self):\n",
    "        \"\"\"Replace contractions in string of text\"\"\"\n",
    "        self.words = contractions.fix(self.words)\n",
    "\n",
    "    def remove_non_ascii(words):\n",
    "        \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def to_lowercase(words):\n",
    "        \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = word.lower()\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def remove_punctuation(words):\n",
    "        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def replace_numbers(words):\n",
    "        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "        p = inflect.engine()\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word.isdigit():\n",
    "                new_word = p.number_to_words(word)\n",
    "                new_words.append(new_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "\n",
    "    def remove_stopwords(words):\n",
    "        \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word not in stopwords.words('english'):\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "\n",
    "    def stem_words(words):\n",
    "        \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "        stemmer = LancasterStemmer()\n",
    "        stems = []\n",
    "        for word in words:\n",
    "            stem = stemmer.stem(word)\n",
    "            stems.append(stem)\n",
    "        return stems\n",
    "\n",
    "    def lemmatize_verbs(words):\n",
    "        \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = []\n",
    "        for word in words:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "            lemmas.append(lemma)\n",
    "        return lemmas\n",
    "\n",
    "    def normalize(words):\n",
    "        words = remove_non_ascii(words)\n",
    "        words = to_lowercase(words)\n",
    "        words = remove_punctuation(words)\n",
    "        words = replace_numbers(words)\n",
    "        words = remove_stopwords(words)\n",
    "        return words\n",
    "\n",
    "count = 0\n",
    "for idx, row in df.iterrows():\n",
    "    print(row['tokenized_text'])\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        break\n",
    "        \n",
    "        \n",
    "# tt = TweetTokenizer()\n",
    "# df['tokenized_text'] = df['text'].apply(tt.tokenize) \n",
    "\n",
    "    \n",
    "# print(type(sample))\n",
    "# words = NormalizationPipeline(sample).process()\n",
    "# print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   user_id        user_key  retweet_count  retweeted  \\\n",
      "648873  962086284209655808        jckungfu              0      False   \n",
      "21860           1727482238  melvinsroberts              0       True   \n",
      "413835  962008285732720640   RayWoodson925              0      False   \n",
      "703988  962100933717983234      marymac169              0      False   \n",
      "678739  962094208071827456        tramarie              0      False   \n",
      "\n",
      "        favorite_count                                               text  \\\n",
      "648873               0  b\"@POTUS @realDonaldTrump\\nRemember this tweet...   \n",
      "21860                0  RT @EvolvedGuppy: @tlcprincess @bunnyhugger75 ...   \n",
      "413835               0  b'@TrueFactsStated The one time I wish someone...   \n",
      "703988               0  b\"\\xe2\\x80\\xbc\\xef\\xb8\\x8f\\xc2\\xa0This is HUGE...   \n",
      "678739               0  b'hey guys sorry, just getting online, was wat...   \n",
      "\n",
      "                  tweet_id  retweeted_status_id  class  \\\n",
      "648873  962086284209655808                    0    NaN   \n",
      "21860   814131178403983360                    0    1.0   \n",
      "413835  962008285732720640                    0    NaN   \n",
      "703988  962100933717983234   962083781313142784    NaN   \n",
      "678739  962094208071827456   961818729632874496    NaN   \n",
      "\n",
      "                                           tokenized_text  \n",
      "648873  [b, \", @POTUS, @realDonaldTrump, \\, nRemember,...  \n",
      "21860   [RT, @EvolvedGuppy, :, @tlcprincess, @bunnyhug...  \n",
      "413835  [b, ', @TrueFactsStated, The, one, time, I, wi...  \n",
      "703988  [b, \", \\, xe2, \\, x80, \\, xbc, \\, xef, \\, xb8,...  \n",
      "678739  [b'hey, guys, sorry, ,, just, getting, online,...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Need to create features for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Diversity\n",
    "def lexical_diversity(text):\n",
    "    if len(text) == 0:\n",
    "        diversity = 0\n",
    "    else: \n",
    "        diversity = float(len(set(text))) / len(text)\n",
    "    return diversity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sklearn.utils.shuffle(df)\n",
    "X = df.iloc[:,0:7]\n",
    "Y = df.iloc[:,8]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'b\\'RT @mitchellvii: UNITE THE BASE! YourVoice\\\\xc2\\\\x99 America (2/8) \"Uranium One Connects Obama!\" https://t.co/c8ga2kr7Hy\\''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/dev/ML/big-data-twitter/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ML/big-data-twitter/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'b\\'RT @mitchellvii: UNITE THE BASE! YourVoice\\\\xc2\\\\x99 America (2/8) \"Uranium One Connects Obama!\" https://t.co/c8ga2kr7Hy\\''"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RFModel = RandomForestClassifier(\n",
    "    n_estimators = 1000, \n",
    "    max_depth = 5, \n",
    "    max_features = 3, \n",
    "    oob_score=False\n",
    ")\n",
    "\n",
    "RFModel.fit(X_train, Y_train)\n",
    "prediction = RFModel.predict_proba(X_test)\n",
    "auc = roc_auc_score(Y_test, prediction[:,1:2])\n",
    "print(auc)\n",
    "\n",
    "RFModel.fit(X_test, Y_test)\n",
    "prediction = RFModel.predict_proba(X_train)\n",
    "auc = roc_auc_score(Y_train, prediction[:,1:2])\n",
    "print(auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
