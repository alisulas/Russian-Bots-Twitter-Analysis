{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMOD 5410 - Big Data\n",
    "## Project: Bot Detection\n",
    "### By: Matt Emmons (0221920)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Write stuff about this project. 3k+ words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.utils\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(101)\n",
    "rand_seed = 101\n",
    "\n",
    "n_rows = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ROOT = \"./csv\"\n",
    "df_nbc = pd.read_csv(\n",
    "    ROOT + \"/tweets.csv\", \n",
    "    encoding='utf-8', \n",
    "    low_memory=False, \n",
    "    parse_dates=False\n",
    ")\n",
    "\n",
    "df_scraped = pd.read_csv(\n",
    "    ROOT + \"/scraped_tweets.csv\", \n",
    "    encoding='utf-8', \n",
    "    nrows=n_rows,\n",
    "    low_memory=False, \n",
    "    parse_dates=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Column'] = df['Column'].str.decode('ascii')\n",
    "# df_scraped['text'] = df_scraped['text'].astype(str).apply(decode(encoding = 'utf-8'))\n",
    "# print(df_scraped['text'].head())\n",
    "# print(df_nbc['text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Let's remove unneeded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "delcols = [\n",
    "    'created_at',\n",
    "    'created_str',\n",
    "    'expanded_urls',\n",
    "    'in_reply_to_status_id',\n",
    "    'source'\n",
    "]\n",
    "for col in delcols:\n",
    "    del df_nbc[col]\n",
    "    del df_scraped[col]\n",
    "    \n",
    "del df_nbc['posted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to ensure all columns have consistent datatypes between the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nbc[['user_id', 'retweet_count', 'favorite_count', 'tweet_id', 'retweeted_status_id']] = df_nbc[['user_id', 'retweet_count', 'favorite_count', 'tweet_id', 'retweeted_status_id']].fillna(0).astype(int)\n",
    "df_nbc[['user_key', 'text']] = df_nbc[['user_key', 'text']].astype('str')\n",
    "df_nbc[['retweeted']] = df_nbc[['retweeted']].astype('bool')\n",
    "df_scraped[['retweeted_status_id']] = df_scraped[['retweeted_status_id']].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id                 int64\n",
      "user_key               object\n",
      "retweet_count           int64\n",
      "retweeted                bool\n",
      "favorite_count          int64\n",
      "text                   object\n",
      "tweet_id                int64\n",
      "hashtags               object\n",
      "mentions               object\n",
      "retweeted_status_id     int64\n",
      "dtype: object\n",
      "user_id                 int64\n",
      "user_key               object\n",
      "retweet_count           int64\n",
      "retweeted                bool\n",
      "favorite_count          int64\n",
      "text                   object\n",
      "tweet_id                int64\n",
      "hashtags               object\n",
      "mentions               object\n",
      "retweeted_status_id     int64\n",
      "dtype: object\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(df_nbc.dtypes)\n",
    "print(df_scraped.dtypes)\n",
    "\n",
    "print(list(df_nbc.dtypes) == list(df_scraped.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id         user_key  retweet_count  retweeted  favorite_count  \\\n",
      "0  2532611755        kathiemrr              0       True               0   \n",
      "1  2531159968   traceyhappymom              0       True               0   \n",
      "2           0    evewebster373              0       True               0   \n",
      "3  4840551713      blacktolive             18      False              17   \n",
      "4  1694026190  jacquelinisbest              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0    #ThingsDoneByMistake kissing auntie in the lips  836227891897651200   \n",
      "1  RT @mc_derpin: #TheOlderWeGet the more pessimi...  765198948239810560   \n",
      "2  RT @dmataconis: Ready To Feel Like A Failure? ...  616002306572746752   \n",
      "3    Amen! #blacklivesmatter https://t.co/wGffaOqgzl  776693302926147584   \n",
      "4  RT @NahBabyNah: Twitchy: Chuck Todd caught out...  777594647875059712   \n",
      "\n",
      "                  hashtags        mentions  retweeted_status_id  \n",
      "0  [\"ThingsDoneByMistake\"]              []                    0  \n",
      "1        [\"TheOlderWeGet\"]              []                    0  \n",
      "2                       []              []                    0  \n",
      "3     [\"Blacklivesmatter\"]              []                    0  \n",
      "4        [\"WakeUpAmerica\"]  [\"nahbabynah\"]   777591478206029824  \n",
      "\n",
      "---------------------\n",
      "\n",
      "              user_id         user_key  retweet_count  retweeted  \\\n",
      "0  961701459900342272  Jeannie22757716              0      False   \n",
      "1  961701477164105728        _p_body__              0      False   \n",
      "2  961701487507312641   6728FixerUpper              0      False   \n",
      "3  961701511289073665        LenAulett              0      False   \n",
      "4  961701912880988161       Dsquared69              0      False   \n",
      "\n",
      "   favorite_count                                               text  \\\n",
      "0               0  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...   \n",
      "1               0  b'\"We do not deserve any right \\xe2\\x80\\x94 an...   \n",
      "2               0  b'Just say no to Martha McSally, who voted for...   \n",
      "3               0  b'Pelosi Thanks Illegals for BREAKING THE LAW,...   \n",
      "4               0  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...   \n",
      "\n",
      "             tweet_id hashtags             mentions  retweeted_status_id  \n",
      "0  961701459900342272       []         ['bbusa617']   961624822953512960  \n",
      "1  961701477164105728       []      ['nprpolitics']   961390308553576448  \n",
      "2  961701487507312641       []  ['LarrySchweikart']   960538375546593280  \n",
      "3  961701511289073665       []         ['bbusa617']   961604790689165312  \n",
      "4  961701912880988161       []         ['bbusa617']   961624822953512960  \n"
     ]
    }
   ],
   "source": [
    "print(df_nbc.head())\n",
    "print(\"\\n---------------------\\n\")\n",
    "print(df_scraped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "\n",
    "What we need to do is this:\n",
    "\n",
    "- Attach a class feature to the nbc dataset\n",
    "- Since we do not know the class of the scraped dataset, we leave it for now\n",
    "- Create a new dataset merged between a subset of nbc and scraped datasets to be our training set\n",
    "- All rows left out of the merged subset will become the test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nbc['class'] = 1\n",
    "df_test = df_scraped.copy()\n",
    "df_test['class'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id         user_key  retweet_count  retweeted  favorite_count  \\\n",
      "0  2532611755        kathiemrr              0       True               0   \n",
      "1  2531159968   traceyhappymom              0       True               0   \n",
      "2           0    evewebster373              0       True               0   \n",
      "3  4840551713      blacktolive             18      False              17   \n",
      "4  1694026190  jacquelinisbest              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0    #ThingsDoneByMistake kissing auntie in the lips  836227891897651200   \n",
      "1  RT @mc_derpin: #TheOlderWeGet the more pessimi...  765198948239810560   \n",
      "2  RT @dmataconis: Ready To Feel Like A Failure? ...  616002306572746752   \n",
      "3    Amen! #blacklivesmatter https://t.co/wGffaOqgzl  776693302926147584   \n",
      "4  RT @NahBabyNah: Twitchy: Chuck Todd caught out...  777594647875059712   \n",
      "\n",
      "                  hashtags        mentions  retweeted_status_id  class  \n",
      "0  [\"ThingsDoneByMistake\"]              []                    0    1.0  \n",
      "1        [\"TheOlderWeGet\"]              []                    0    1.0  \n",
      "2                       []              []                    0    1.0  \n",
      "3     [\"Blacklivesmatter\"]              []                    0    1.0  \n",
      "4        [\"WakeUpAmerica\"]  [\"nahbabynah\"]   777591478206029824    1.0  \n",
      "user_id                  int64\n",
      "user_key                object\n",
      "retweet_count            int64\n",
      "retweeted                 bool\n",
      "favorite_count           int64\n",
      "text                    object\n",
      "tweet_id                 int64\n",
      "hashtags                object\n",
      "mentions                object\n",
      "retweeted_status_id      int64\n",
      "class                  float64\n",
      "dtype: object\n",
      "1203451\n"
     ]
    }
   ],
   "source": [
    "# Merge df_test and df_nbc\n",
    "# train/test/val split\n",
    "# apply classifiers\n",
    "df = pd.concat([df_nbc, df_test], ignore_index = True)\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "print(len(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "headers = list(df.columns.values)\n",
    "headers.extend(['tokenized_text', 'stem_text', 'lemma_text'])\n",
    "with open(\".\\\\csv\\\\mergedtweets.csv\", 'w', encoding = 'utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames = headers, lineterminator = '\\n')\n",
    "        writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TweetTokenizer()\n",
    "\n",
    "def preprocess_text(tokenized_text):\n",
    "    \"\"\"\n",
    "    Accepts an array of strings\n",
    "    Handles all the preprocessing of the tokenized text before stemming/lemma\n",
    "    - removes 'b'\n",
    "    - removes unicode symbols\n",
    "    - replaces contractions\n",
    "    - removes non-ascii symbols\n",
    "    - converts to lowercase\n",
    "    - removes punctuation\n",
    "    - removes numbers\n",
    "    \"\"\"\n",
    "    new_words = []\n",
    "    for word in tokenized_text:\n",
    "        # remove 'b' from tokens\n",
    "        if word == tokenized_text[0]:\n",
    "            if len(word) == 1 and word[0] == 'b':\n",
    "                continue\n",
    "            elif len(word) > 1 and word[:2] == \"b'\":\n",
    "                word = word[2:]\n",
    "                \n",
    "        # remove unicode symbols from tokens\n",
    "        word = re.sub(r\"(x[abcdef0-9]{0,2})?\", '', word)\n",
    "        # word = remove_unicode(word)\n",
    "        \n",
    "        # replace contractions from tokens\n",
    "        word = contractions.fix(word)\n",
    "        # word = replace_contractions(word)\n",
    "        \n",
    "        # remove non-ascii from tokens\n",
    "        word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        # word = remove_non_ascii(word)\n",
    "        \n",
    "        # convert word to lowercase\n",
    "        word = word.lower()\n",
    "        \n",
    "        # remove links from token\n",
    "        word = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', word)\n",
    "        # word = remove_links(word)\n",
    "        \n",
    "        # remove punctuation if it results in an actual world\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '' or new_word != None:\n",
    "            word = new_word\n",
    "            \n",
    "        # replace numbers with textual representation\n",
    "        word = replace_numbers(word)\n",
    "        \n",
    "        # skip over errant 'n'\n",
    "        if len(word) == 1 and word[0] == 'n':\n",
    "            continue\n",
    "            \n",
    "        # skip over all stopwords\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_unicode(word):\n",
    "    new_word = re.sub(r\"(x[abcdef0-9]{0,2})?\", '', word)\n",
    "    return new_word\n",
    "\n",
    "def replace_contractions(word):\n",
    "    new_word = contractions.fix(word)\n",
    "    return new_word\n",
    "\n",
    "def remove_non_ascii(word):\n",
    "    new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_word\n",
    "\n",
    "def remove_links(word):\n",
    "    new_word = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', word)\n",
    "    return new_word\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    if new_word != '':\n",
    "        return new_word\n",
    "    \n",
    "def replace_numbers(word):\n",
    "    if word.isdigit():\n",
    "        p = inflect.engine()\n",
    "        new_word = p.number_to_words(word)\n",
    "        return new_word\n",
    "        new_words.append(new_word)\n",
    "    else:\n",
    "        return word \n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def handle_text(row):\n",
    "    \"\"\"\n",
    "    Tokenize, Stem and Lemma the text.\n",
    "    Append these values, in that order, to row.values\n",
    "    \"\"\"\n",
    "    text = row['text']\n",
    "    tokenized = tt.tokenize(text)\n",
    "    processed = preprocess_text(tokenized)\n",
    "    stems = stem_words(processed)\n",
    "    lemmas = lemmatize_words(processed)\n",
    "#     print(\"\\nTokenized: {}\".format(tokenized))\n",
    "#     print(\"Stems: {}\".format(stems))\n",
    "#     print(\"Lemmas: {}\\n\".format(lemmas))\n",
    "    return [tokenized, stems, lemmas]\n",
    "    \n",
    "def write_to_csv(row):\n",
    "    with open(\".\\\\csv\\\\mergedtweets.csv\", 'a', encoding = 'utf-8', newline = '') as file:\n",
    "        writer = csv.writer(file, delimiter = ',')\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 518, 5901, 9102, 300005, 401509, 567891, 991858, 991500, 991918]\n",
    "\n",
    "def analyze():\n",
    "    with open(\".\\\\csv\\\\mergedtweets.csv\", 'a', encoding = 'utf-8', newline = '') as file:\n",
    "        writer = csv.writer(file, delimiter = ',')\n",
    "        for index, row in tqdm(df.iterrows()):\n",
    "            augmented = handle_text(row)\n",
    "            values = list(row.values)\n",
    "            values.extend(augmented)\n",
    "            writer.writerow(values)\n",
    "\n",
    "analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: b'CRAZZZZY TOWN !!\\n\\n1ST SCHIFF AND WARREN FOILED  \\xe2\\x80\\x94 NOW OBAMA\\xe2\\x80\\x99S FBI BUSTED TRYING TO PAY RUSSIANS 1 MILLION FOR DIRT ON TRUMP !!\\n\\n\\xe2\\x80\\x9cWhat is clear is the Obama Administration spied on a rival political campaign.\\xe2\\x80\\x9d \\xe2\\x80\\x94Tucker\\n\\nLOCK THEM UP !!! https://t.co/EQkZclNcbK'\n",
      "Lemmas: ['crazzzzy', 'town', 'n', 'n1st', 'schiff', 'warren', 'foil', 'obama', 'fbi', 'bust', 'try', 'pay', 'russians', 'one', 'million', 'dirt', 'trump', 'n', 'n', 'clear', 'obama', 'administration', 'spy', 'rival', 'political', 'campaign', 'tucker', 'n', 'nlock']\n",
      "Stems: ['crazzzzy', 'town', 'n', 'n1st', 'schiff', 'war', 'foil', 'obam', 'fbi', 'bust', 'try', 'pay', 'russ', 'on', 'mil', 'dirt', 'trump', 'n', 'n', 'clear', 'obam', 'admin', 'spi', 'riv', 'polit', 'campaign', 'tuck', 'n', 'nlock']\n",
      "\n",
      "----\n",
      "\n",
      "Text: b'Trump blocks release of Russia memo drafted by Democrats https://t.co/T4W4BbicfQ\\n#ReleaseTheMemo\\n#ReleaseTheDemMemo\\n#ReleaseTheDemsMemo https://t.co/1mBh7FvThU'\n",
      "Lemmas: ['trump', 'block', 'release', 'russia', 'memo', 'draft', 'democrats']\n",
      "Stems: ['trump', 'block', 'releas', 'russ', 'memo', 'draft', 'democr']\n",
      "\n",
      "----\n",
      "\n",
      "Text: b'CRAZZZZY TOWN !!\\n\\n1ST SCHIFF AND WARREN FOILED  \\xe2\\x80\\x94 NOW OBAMA\\xe2\\x80\\x99S FBI BUSTED TRYING TO PAY RUSSIANS 1 MILLION FOR DIRT ON TRUMP !!\\n\\n\\xe2\\x80\\x9cWhat is clear is the Obama Administration spied on a rival political campaign.\\xe2\\x80\\x9d \\xe2\\x80\\x94Tucker\\n\\nLOCK THEM UP !!! https://t.co/EQkZclNcbK'\n",
      "Lemmas: ['crazzzzy', 'town', 'n', 'n1st', 'schiff', 'warren', 'foil', 'obama', 'fbi', 'bust', 'try', 'pay', 'russians', 'one', 'million', 'dirt', 'trump', 'n', 'n', 'clear', 'obama', 'administration', 'spy', 'rival', 'political', 'campaign', 'tucker', 'n', 'nlock']\n",
      "Stems: ['crazzzzy', 'town', 'n', 'n1st', 'schiff', 'war', 'foil', 'obam', 'fbi', 'bust', 'try', 'pay', 'russ', 'on', 'mil', 'dirt', 'trump', 'n', 'n', 'clear', 'obam', 'admin', 'spi', 'riv', 'polit', 'campaign', 'tuck', 'n', 'nlock']\n",
      "\n",
      "----\n",
      "\n",
      "Text: b'Reminder as Trump says he\\'s unable to declassify the Dems\\' memo: DOJ publicly expressed \"grave concern\" over the Nunes memo and Trump released it anyway. DOJ has not expressed concern, at least not publicly, over the Dems\\' memo.'\n",
      "Lemmas: ['reminder', 'trump', 'say', 'unable', 'declassify', 'dems', 'memo', 'doj', 'publicly', 'epressed', 'grave', 'concern', 'nunes', 'memo', 'trump', 'release', 'anyway', 'doj', 'epressed', 'concern', 'least', 'publicly', 'dems', 'memo']\n",
      "Stems: ['remind', 'trump', 'say', 'un', 'declass', 'dem', 'memo', 'doj', 'publ', 'epress', 'grav', 'concern', 'nun', 'memo', 'trump', 'releas', 'anyway', 'doj', 'epress', 'concern', 'least', 'publ', 'dem', 'memo']\n",
      "\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [991858, 991500, 991858, 991918]\n",
    "\n",
    "for sample in samples:\n",
    "    tokenized = df.iloc[sample].tokenized_text\n",
    "    text = df.iloc[sample].text\n",
    "    stems, lemmas = normalize(tokenized)\n",
    "    print(\"Text: {}\".format(text))\n",
    "    print(\"Lemmas: {}\".format(lemmas))\n",
    "    print(\"Stems: {}\".format(stems))\n",
    "    print(\"\\n----\\n\")\n",
    "    \n",
    "# df['stem_text'] = df['tokenized_text'].apply(get_stems)\n",
    "# df['lemma_text'] = df['tokenized_text'].apply(get_lemmas)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id         user_key  retweet_count  retweeted  favorite_count  \\\n",
      "0  2532611755        kathiemrr              0       True               0   \n",
      "1  2531159968   traceyhappymom              0       True               0   \n",
      "2           0    evewebster373              0       True               0   \n",
      "3  4840551713      blacktolive             18      False              17   \n",
      "4  1694026190  jacquelinisbest              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0    #ThingsDoneByMistake kissing auntie in the lips  836227891897651200   \n",
      "1  RT @mc_derpin: #TheOlderWeGet the more pessimi...  765198948239810560   \n",
      "2  RT @dmataconis: Ready To Feel Like A Failure? ...  616002306572746752   \n",
      "3    Amen! #blacklivesmatter https://t.co/wGffaOqgzl  776693302926147584   \n",
      "4  RT @NahBabyNah: Twitchy: Chuck Todd caught out...  777594647875059712   \n",
      "\n",
      "                  hashtags        mentions  retweeted_status_id  class  \\\n",
      "0  [\"ThingsDoneByMistake\"]              []                    0    1.0   \n",
      "1        [\"TheOlderWeGet\"]              []                    0    1.0   \n",
      "2                       []              []                    0    1.0   \n",
      "3     [\"Blacklivesmatter\"]              []                    0    1.0   \n",
      "4        [\"WakeUpAmerica\"]  [\"nahbabynah\"]   777591478206029824    1.0   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0  [#ThingsDoneByMistake, kissing, auntie, in, th...  \n",
      "1  [RT, @mc_derpin, :, #TheOlderWeGet, the, more,...  \n",
      "2  [RT, @dmataconis, :, Ready, To, Feel, Like, A,...  \n",
      "3  [Amen, !, #blacklivesmatter, https://t.co/wGff...  \n",
      "4  [RT, @NahBabyNah, :, Twitchy, :, Chuck, Todd, ...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Need to create features for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Diversity\n",
    "def lexical_diversity(text):\n",
    "    if len(text) == 0:\n",
    "        diversity = 0\n",
    "    else: \n",
    "        diversity = float(len(set(text))) / len(text)\n",
    "    return diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sklearn.utils.shuffle(df)\n",
    "X = df.iloc[:,0:7]\n",
    "Y = df.iloc[:,8]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'b\\'RT @mitchellvii: UNITE THE BASE! YourVoice\\\\xc2\\\\x99 America (2/8) \"Uranium One Connects Obama!\" https://t.co/c8ga2kr7Hy\\''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/dev/ML/big-data-twitter/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ML/big-data-twitter/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'b\\'RT @mitchellvii: UNITE THE BASE! YourVoice\\\\xc2\\\\x99 America (2/8) \"Uranium One Connects Obama!\" https://t.co/c8ga2kr7Hy\\''"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RFModel = RandomForestClassifier(\n",
    "    n_estimators = 1000, \n",
    "    max_depth = 5, \n",
    "    max_features = 3, \n",
    "    oob_score=False\n",
    ")\n",
    "\n",
    "RFModel.fit(X_train, Y_train)\n",
    "prediction = RFModel.predict_proba(X_test)\n",
    "auc = roc_auc_score(Y_test, prediction[:,1:2])\n",
    "print(auc)\n",
    "\n",
    "RFModel.fit(X_test, Y_test)\n",
    "prediction = RFModel.predict_proba(X_train)\n",
    "auc = roc_auc_score(Y_train, prediction[:,1:2])\n",
    "print(auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
