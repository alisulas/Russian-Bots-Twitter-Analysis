{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.utils\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "np.random.seed(101)\n",
    "rand_seed = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ROOT = \"./csv\"\n",
    "df = pd.read_csv(\n",
    "    ROOT + \"/propertweets.csv\", \n",
    "    encoding='utf-8', \n",
    "    low_memory=False, \n",
    "    parse_dates=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000000, 18)\n",
      "user_id                   int64\n",
      "user_key                 object\n",
      "created_at                int64\n",
      "created_str              object\n",
      "retweet_count             int64\n",
      "retweeted                  bool\n",
      "favorite_count            int64\n",
      "text                     object\n",
      "tweet_id                  int64\n",
      "source                   object\n",
      "hashtags                 object\n",
      "expanded_urls            object\n",
      "mentions                 object\n",
      "retweeted_status_id       int64\n",
      "in_reply_to_status_id     int64\n",
      "tokens                   object\n",
      "stems                    object\n",
      "lemmas                   object\n",
      "dtype: object\n",
      "              user_id         user_key     created_at  \\\n",
      "0  961701459900342272  Jeannie22757716  1518122470246   \n",
      "1  961701477164105728        _p_body__  1518122474362   \n",
      "2  961701487507312641   6728FixerUpper  1518122476828   \n",
      "3  961701511289073665        LenAulett  1518122482498   \n",
      "4  961701912880988161       Dsquared69  1518122578245   \n",
      "\n",
      "                      created_str  retweet_count  retweeted  favorite_count  \\\n",
      "0  Thu Feb 08 20:41:10 +0000 2018              0      False               0   \n",
      "1  Thu Feb 08 20:41:14 +0000 2018              0      False               0   \n",
      "2  Thu Feb 08 20:41:16 +0000 2018              0      False               0   \n",
      "3  Thu Feb 08 20:41:22 +0000 2018              0      False               0   \n",
      "4  Thu Feb 08 20:42:58 +0000 2018              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...  961701459900342272   \n",
      "1  b'\"We do not deserve any right \\xe2\\x80\\x94 an...  961701477164105728   \n",
      "2  b'Just say no to Martha McSally, who voted for...  961701487507312641   \n",
      "3  b'Pelosi Thanks Illegals for BREAKING THE LAW,...  961701511289073665   \n",
      "4  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...  961701912880988161   \n",
      "\n",
      "                                              source hashtags  \\\n",
      "0  <a href=\"http://twitter.com/download/android\" ...       []   \n",
      "1  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       []   \n",
      "2  <a href=\"http://twitter.com/download/android\" ...       []   \n",
      "3  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...       []   \n",
      "4  <a href=\"http://twitter.com/download/iphone\" r...       []   \n",
      "\n",
      "                                       expanded_urls             mentions  \\\n",
      "0                                                 []         ['bbusa617']   \n",
      "1                                                 []      ['nprpolitics']   \n",
      "2                                                 []  ['LarrySchweikart']   \n",
      "3  ['http://truthfeednews.com/pelosi-thanks-illeg...         ['bbusa617']   \n",
      "4                                                 []         ['bbusa617']   \n",
      "\n",
      "   retweeted_status_id  in_reply_to_status_id  \\\n",
      "0   961624822953512960                      0   \n",
      "1   961390308553576448                      0   \n",
      "2   960538375546593280                      0   \n",
      "3   961604790689165312                      0   \n",
      "4   961624822953512960                      0   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  ['george', 'bush', 'abu', 'dhabi', 'trashes', ...   \n",
      "1  ['deserve', 'love', 'dreamers', 'intention', '...   \n",
      "2  ['maha', 'mcsally', 'voted', 'mcmuffin', 'cank...   \n",
      "3  ['pelosi', 'illegals', 'breaking', 'law', 'cla...   \n",
      "4  ['george', 'bush', 'abu', 'dhabi', 'trashes', ...   \n",
      "\n",
      "                                               stems  \\\n",
      "0  ['georg', 'bush', 'abu', 'dhabi', 'trash', 'tr...   \n",
      "1  ['deserv', 'love', 'dreamer', 'intent', 'pelosi']   \n",
      "2  ['maha', 'mcsalli', 'vote', 'mcmuffin', 'cankl...   \n",
      "3  ['pelosi', 'illeg', 'break', 'law', 'claim', '...   \n",
      "4  ['georg', 'bush', 'abu', 'dhabi', 'trash', 'tr...   \n",
      "\n",
      "                                              lemmas  \n",
      "0  ['george', 'bush', 'abu', 'dhabi', 'trash', 't...  \n",
      "1  ['deserve', 'love', 'dreamers', 'intention', '...  \n",
      "2  ['maha', 'mcsally', 'vote', 'mcmuffin', 'cankl...  \n",
      "3  ['pelosi', 'illegals', 'break', 'law', 'claim'...  \n",
      "4  ['george', 'bush', 'abu', 'dhabi', 'trash', 't...  \n"
     ]
    }
   ],
   "source": [
    "del df['Unnamed: 0']\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    if len(text) == 0:\n",
    "        diversity = 0\n",
    "    else: \n",
    "        diversity = float(len(set(text))) / len(text)\n",
    "    return diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    user_id         user_key     created_at  \\\n",
      "2203446  963619824265023488        aviviavai  1518579843952   \n",
      "2203447  963619824503894016  davidinkuwait69  1518579844009   \n",
      "2203448  963619824768376833     trumpliesbot  1518579844072   \n",
      "2203449  963619825229611008        SteveoUSA  1518579844182   \n",
      "2203450  963619825036783618       RichieRoby  1518579844136   \n",
      "\n",
      "                            created_str  retweet_count  retweeted  \\\n",
      "2203446  Wed Feb 14 03:44:03 +0000 2018              0      False   \n",
      "2203447  Wed Feb 14 03:44:04 +0000 2018              0      False   \n",
      "2203448  Wed Feb 14 03:44:04 +0000 2018              0      False   \n",
      "2203449  Wed Feb 14 03:44:04 +0000 2018              0      False   \n",
      "2203450  Wed Feb 14 03:44:04 +0000 2018              0      False   \n",
      "\n",
      "         favorite_count                                               text  \\\n",
      "2203446               0  b'RT @NicCageMatch: White People Once Kept Bla...   \n",
      "2203447               0  b'The Ex Resident Obama used Kehinde Wiley to ...   \n",
      "2203448               0  b\"Come on #MAGA, admit it. Trump and his co-co...   \n",
      "2203449               0  b'History will show that @GenFlynn was a patri...   \n",
      "2203450               0  b'Not sure how one does better than applying t...   \n",
      "\n",
      "                   tweet_id  \\\n",
      "2203446  963619824265023488   \n",
      "2203447  963619824503894016   \n",
      "2203448  963619824768376833   \n",
      "2203449  963619825229611008   \n",
      "2203450  963619825036783618   \n",
      "\n",
      "                                                    source       ...        \\\n",
      "2203446  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...       ...         \n",
      "2203447  <a href=\"http://twitter.com/download/android\" ...       ...         \n",
      "2203448  <a href=\"http://twitter.com/trumpliesbot\" rel=...       ...         \n",
      "2203449  <a href=\"http://www.twitter.com\" rel=\"nofollow...       ...         \n",
      "2203450  <a href=\"https://about.twitter.com/products/tw...       ...         \n",
      "\n",
      "                                             expanded_urls  \\\n",
      "2203446  ['https://twitter.com/thewrap/status/963161707...   \n",
      "2203447                                                 []   \n",
      "2203448                                                 []   \n",
      "2203449                                                 []   \n",
      "2203450  ['https://twitter.com/i/web/status/96361982503...   \n",
      "\n",
      "                             mentions retweeted_status_id  \\\n",
      "2203446              ['NicCageMatch']  963170112567201792   \n",
      "2203447              ['ElderLansing']  963249595949158400   \n",
      "2203448               ['OMGno2trump']  963607817839153152   \n",
      "2203449  ['johncardillo', 'GenFlynn']  963394200069951488   \n",
      "2203450                            []                   0   \n",
      "\n",
      "         in_reply_to_status_id  class  \\\n",
      "2203446                      0    NaN   \n",
      "2203447                      0    NaN   \n",
      "2203448                      0    NaN   \n",
      "2203449                      0    NaN   \n",
      "2203450                      0    NaN   \n",
      "\n",
      "                                            tokenized_text  \\\n",
      "2203446  [\"b'RT\", '@NicCageMatch', ':', 'White', 'Peopl...   \n",
      "2203447  [\"b'The\", 'Ex', 'Resident', 'Obama', 'used', '...   \n",
      "2203448  ['b', '\"', 'Come', 'on', '#MAGA', ',', 'admit'...   \n",
      "2203449  [\"b'History\", 'will', 'show', 'that', '@GenFly...   \n",
      "2203450  [\"b'Not\", 'sure', 'how', 'one', 'does', 'bette...   \n",
      "\n",
      "                                                 stem_text  \\\n",
      "2203446  ['rt', 'niccagematch', '', 'whit', 'peopl', 'k...   \n",
      "2203447  ['e', 'resid', 'obam', 'us', 'kehind', 'wiley'...   \n",
      "2203448  ['', 'com', 'mag', '', 'admit', '', 'trump', '...   \n",
      "2203449  ['hist', 'show', 'genflyn', 'patriot', 'malicy...   \n",
      "2203450  ['sur', 'on', 'bet', 'apply', 'rul', 'law', 'i...   \n",
      "\n",
      "                                                lemma_text lemma_diversity  \\\n",
      "2203446  ['rt', 'niccagematch', '', 'white', 'people', ...        0.258427   \n",
      "2203447  ['e', 'resident', 'obama', 'use', 'kehinde', '...        0.083871   \n",
      "2203448  ['', 'come', 'maga', '', 'admit', '', 'trump',...        0.099338   \n",
      "2203449  ['history', 'show', 'genflynn', 'patriot', 'ma...        0.117647   \n",
      "2203450  ['sure', 'one', 'better', 'apply', 'rule', 'la...        0.164384   \n",
      "\n",
      "         stem_diversity  \n",
      "2203446        0.270588  \n",
      "2203447        0.094891  \n",
      "2203448        0.104167  \n",
      "2203449        0.131980  \n",
      "2203450        0.183206  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "df['lemma_diversity'] = df['lemma_text'].apply(lexical_diversity)\n",
    "df['stem_diversity'] = df['stem_text'].apply(lexical_diversity)\n",
    "\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf: term frequency-inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a5dbbe32745a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m    812\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, analyzer='word', ngram_range=(1, 2), stop_words='english')\n",
    "vz = vectorizer.fit_transform(list(df['tokens'].map(lambda tokens: ' '.join(tokens))))\n",
    "\n",
    "vz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "tfidf = pd.DataFrame(columns=['tfidf']).from_dict(dict(tfidf), orient='index')\n",
    "tfidf.columns = ['tfidf']\n",
    "tfidf.tfidf.hist(bins=25, figsize=(15,7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
