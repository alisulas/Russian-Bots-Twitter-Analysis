{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.utils\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "np.random.seed(101)\n",
    "rand_seed = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ROOT = \"./csv\"\n",
    "df = pd.read_csv(\n",
    "    ROOT + \"/propertweets.csv\", \n",
    "    encoding='utf-8', \n",
    "    low_memory=False, \n",
    "    parse_dates=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000000, 19)\n",
      "Unnamed: 0                int64\n",
      "user_id                   int64\n",
      "user_key                 object\n",
      "created_at                int64\n",
      "created_str              object\n",
      "retweet_count             int64\n",
      "retweeted                  bool\n",
      "favorite_count            int64\n",
      "text                     object\n",
      "tweet_id                  int64\n",
      "source                   object\n",
      "hashtags                 object\n",
      "expanded_urls            object\n",
      "mentions                 object\n",
      "retweeted_status_id       int64\n",
      "in_reply_to_status_id     int64\n",
      "tokens                   object\n",
      "stems                    object\n",
      "lemmas                   object\n",
      "dtype: object\n",
      "   Unnamed: 0             user_id         user_key     created_at  \\\n",
      "0           0  961701459900342272  Jeannie22757716  1518122470246   \n",
      "1           1  961701477164105728        _p_body__  1518122474362   \n",
      "2           2  961701487507312641   6728FixerUpper  1518122476828   \n",
      "3           3  961701511289073665        LenAulett  1518122482498   \n",
      "4           4  961701912880988161       Dsquared69  1518122578245   \n",
      "\n",
      "                      created_str  retweet_count  retweeted  favorite_count  \\\n",
      "0  Thu Feb 08 20:41:10 +0000 2018              0      False               0   \n",
      "1  Thu Feb 08 20:41:14 +0000 2018              0      False               0   \n",
      "2  Thu Feb 08 20:41:16 +0000 2018              0      False               0   \n",
      "3  Thu Feb 08 20:41:22 +0000 2018              0      False               0   \n",
      "4  Thu Feb 08 20:42:58 +0000 2018              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...  961701459900342272   \n",
      "1  b'\"We do not deserve any right \\xe2\\x80\\x94 an...  961701477164105728   \n",
      "2  b'Just say no to Martha McSally, who voted for...  961701487507312641   \n",
      "3  b'Pelosi Thanks Illegals for BREAKING THE LAW,...  961701511289073665   \n",
      "4  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...  961701912880988161   \n",
      "\n",
      "                                              source hashtags  \\\n",
      "0  <a href=\"http://twitter.com/download/android\" ...       []   \n",
      "1  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       []   \n",
      "2  <a href=\"http://twitter.com/download/android\" ...       []   \n",
      "3  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...       []   \n",
      "4  <a href=\"http://twitter.com/download/iphone\" r...       []   \n",
      "\n",
      "                                       expanded_urls             mentions  \\\n",
      "0                                                 []         ['bbusa617']   \n",
      "1                                                 []      ['nprpolitics']   \n",
      "2                                                 []  ['LarrySchweikart']   \n",
      "3  ['http://truthfeednews.com/pelosi-thanks-illeg...         ['bbusa617']   \n",
      "4                                                 []         ['bbusa617']   \n",
      "\n",
      "   retweeted_status_id  in_reply_to_status_id  \\\n",
      "0   961624822953512960                      0   \n",
      "1   961390308553576448                      0   \n",
      "2   960538375546593280                      0   \n",
      "3   961604790689165312                      0   \n",
      "4   961624822953512960                      0   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  ['george', 'bush', 'abu', 'dhabi', 'trashes', ...   \n",
      "1  ['deserve', 'love', 'dreamers', 'intention', '...   \n",
      "2  ['maha', 'mcsally', 'voted', 'mcmuffin', 'cank...   \n",
      "3  ['pelosi', 'illegals', 'breaking', 'law', 'cla...   \n",
      "4  ['george', 'bush', 'abu', 'dhabi', 'trashes', ...   \n",
      "\n",
      "                                               stems  \\\n",
      "0  ['georg', 'bush', 'abu', 'dhabi', 'trash', 'tr...   \n",
      "1  ['deserv', 'love', 'dreamer', 'intent', 'pelosi']   \n",
      "2  ['maha', 'mcsalli', 'vote', 'mcmuffin', 'cankl...   \n",
      "3  ['pelosi', 'illeg', 'break', 'law', 'claim', '...   \n",
      "4  ['georg', 'bush', 'abu', 'dhabi', 'trash', 'tr...   \n",
      "\n",
      "                                              lemmas  \n",
      "0  ['george', 'bush', 'abu', 'dhabi', 'trash', 't...  \n",
      "1  ['deserve', 'love', 'dreamers', 'intention', '...  \n",
      "2  ['maha', 'mcsally', 'vote', 'mcmuffin', 'cankl...  \n",
      "3  ['pelosi', 'illegals', 'break', 'law', 'claim'...  \n",
      "4  ['george', 'bush', 'abu', 'dhabi', 'trash', 't...  \n"
     ]
    }
   ],
   "source": [
    "del df_nbc['Unnamed: 0']\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    if len(text) == 0:\n",
    "        diversity = 0\n",
    "    else: \n",
    "        diversity = float(len(set(text))) / len(text)\n",
    "    return diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    user_id         user_key     created_at  \\\n",
      "2203446  963619824265023488        aviviavai  1518579843952   \n",
      "2203447  963619824503894016  davidinkuwait69  1518579844009   \n",
      "2203448  963619824768376833     trumpliesbot  1518579844072   \n",
      "2203449  963619825229611008        SteveoUSA  1518579844182   \n",
      "2203450  963619825036783618       RichieRoby  1518579844136   \n",
      "\n",
      "                            created_str  retweet_count  retweeted  \\\n",
      "2203446  Wed Feb 14 03:44:03 +0000 2018              0      False   \n",
      "2203447  Wed Feb 14 03:44:04 +0000 2018              0      False   \n",
      "2203448  Wed Feb 14 03:44:04 +0000 2018              0      False   \n",
      "2203449  Wed Feb 14 03:44:04 +0000 2018              0      False   \n",
      "2203450  Wed Feb 14 03:44:04 +0000 2018              0      False   \n",
      "\n",
      "         favorite_count                                               text  \\\n",
      "2203446               0  b'RT @NicCageMatch: White People Once Kept Bla...   \n",
      "2203447               0  b'The Ex Resident Obama used Kehinde Wiley to ...   \n",
      "2203448               0  b\"Come on #MAGA, admit it. Trump and his co-co...   \n",
      "2203449               0  b'History will show that @GenFlynn was a patri...   \n",
      "2203450               0  b'Not sure how one does better than applying t...   \n",
      "\n",
      "                   tweet_id  \\\n",
      "2203446  963619824265023488   \n",
      "2203447  963619824503894016   \n",
      "2203448  963619824768376833   \n",
      "2203449  963619825229611008   \n",
      "2203450  963619825036783618   \n",
      "\n",
      "                                                    source       ...        \\\n",
      "2203446  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...       ...         \n",
      "2203447  <a href=\"http://twitter.com/download/android\" ...       ...         \n",
      "2203448  <a href=\"http://twitter.com/trumpliesbot\" rel=...       ...         \n",
      "2203449  <a href=\"http://www.twitter.com\" rel=\"nofollow...       ...         \n",
      "2203450  <a href=\"https://about.twitter.com/products/tw...       ...         \n",
      "\n",
      "                                             expanded_urls  \\\n",
      "2203446  ['https://twitter.com/thewrap/status/963161707...   \n",
      "2203447                                                 []   \n",
      "2203448                                                 []   \n",
      "2203449                                                 []   \n",
      "2203450  ['https://twitter.com/i/web/status/96361982503...   \n",
      "\n",
      "                             mentions retweeted_status_id  \\\n",
      "2203446              ['NicCageMatch']  963170112567201792   \n",
      "2203447              ['ElderLansing']  963249595949158400   \n",
      "2203448               ['OMGno2trump']  963607817839153152   \n",
      "2203449  ['johncardillo', 'GenFlynn']  963394200069951488   \n",
      "2203450                            []                   0   \n",
      "\n",
      "         in_reply_to_status_id  class  \\\n",
      "2203446                      0    NaN   \n",
      "2203447                      0    NaN   \n",
      "2203448                      0    NaN   \n",
      "2203449                      0    NaN   \n",
      "2203450                      0    NaN   \n",
      "\n",
      "                                            tokenized_text  \\\n",
      "2203446  [\"b'RT\", '@NicCageMatch', ':', 'White', 'Peopl...   \n",
      "2203447  [\"b'The\", 'Ex', 'Resident', 'Obama', 'used', '...   \n",
      "2203448  ['b', '\"', 'Come', 'on', '#MAGA', ',', 'admit'...   \n",
      "2203449  [\"b'History\", 'will', 'show', 'that', '@GenFly...   \n",
      "2203450  [\"b'Not\", 'sure', 'how', 'one', 'does', 'bette...   \n",
      "\n",
      "                                                 stem_text  \\\n",
      "2203446  ['rt', 'niccagematch', '', 'whit', 'peopl', 'k...   \n",
      "2203447  ['e', 'resid', 'obam', 'us', 'kehind', 'wiley'...   \n",
      "2203448  ['', 'com', 'mag', '', 'admit', '', 'trump', '...   \n",
      "2203449  ['hist', 'show', 'genflyn', 'patriot', 'malicy...   \n",
      "2203450  ['sur', 'on', 'bet', 'apply', 'rul', 'law', 'i...   \n",
      "\n",
      "                                                lemma_text lemma_diversity  \\\n",
      "2203446  ['rt', 'niccagematch', '', 'white', 'people', ...        0.258427   \n",
      "2203447  ['e', 'resident', 'obama', 'use', 'kehinde', '...        0.083871   \n",
      "2203448  ['', 'come', 'maga', '', 'admit', '', 'trump',...        0.099338   \n",
      "2203449  ['history', 'show', 'genflynn', 'patriot', 'ma...        0.117647   \n",
      "2203450  ['sure', 'one', 'better', 'apply', 'rule', 'la...        0.164384   \n",
      "\n",
      "         stem_diversity  \n",
      "2203446        0.270588  \n",
      "2203447        0.094891  \n",
      "2203448        0.104167  \n",
      "2203449        0.131980  \n",
      "2203450        0.183206  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "df['lemma_diversity'] = df['lemma_text'].apply(lexical_diversity)\n",
    "df['stem_diversity'] = df['stem_text'].apply(lexical_diversity)\n",
    "\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf: term frequency-inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id         user_key     created_at          created_str  \\\n",
      "0  2532611755        kathiemrr  1488207240000  2017-02-27 14:54:00   \n",
      "1  2531159968   traceyhappymom  1471272620000  2016-08-15 14:50:20   \n",
      "2           0    evewebster373  1435701369000  2015-06-30 21:56:09   \n",
      "3  4840551713      blacktolive  1474013088000  2016-09-16 08:04:48   \n",
      "4  1694026190  jacquelinisbest  1474227985000  2016-09-18 19:46:25   \n",
      "\n",
      "   retweet_count  retweeted  favorite_count  \\\n",
      "0              0       True               0   \n",
      "1              0       True               0   \n",
      "2              0       True               0   \n",
      "3             18      False              17   \n",
      "4              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0    #ThingsDoneByMistake kissing auntie in the lips  836227891897651200   \n",
      "1  RT @mc_derpin: #TheOlderWeGet the more pessimi...  765198948239810560   \n",
      "2  RT @dmataconis: Ready To Feel Like A Failure? ...  616002306572746752   \n",
      "3    Amen! #blacklivesmatter https://t.co/wGffaOqgzl  776693302926147584   \n",
      "4  RT @NahBabyNah: Twitchy: Chuck Todd caught out...  777594647875059712   \n",
      "\n",
      "                                              source                 hashtags  \\\n",
      "0                                                NaN  [\"ThingsDoneByMistake\"]   \n",
      "1                                                NaN        [\"TheOlderWeGet\"]   \n",
      "2                                                NaN                       []   \n",
      "3  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...     [\"Blacklivesmatter\"]   \n",
      "4  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...        [\"WakeUpAmerica\"]   \n",
      "\n",
      "                                 expanded_urls        mentions  \\\n",
      "0                                           []              []   \n",
      "1                                           []              []   \n",
      "2                                           []              []   \n",
      "3                                           []              []   \n",
      "4  [\"http://ln.is/twitchy.com/loriz-31/3yafU\"]  [\"nahbabynah\"]   \n",
      "\n",
      "   retweeted_status_id  in_reply_to_status_id  class  \\\n",
      "0                    0                      0    1.0   \n",
      "1                    0                      0    1.0   \n",
      "2                    0                      0    1.0   \n",
      "3                    0                      0    1.0   \n",
      "4   777591478206029824                      0    1.0   \n",
      "\n",
      "                                      tokenized_text  \\\n",
      "0  ['#ThingsDoneByMistake', 'kissing', 'auntie', ...   \n",
      "1  ['RT', '@mc_derpin', ':', '#TheOlderWeGet', 't...   \n",
      "2  ['RT', '@dmataconis', ':', 'Ready', 'To', 'Fee...   \n",
      "3  ['Amen', '!', '#blacklivesmatter', 'https://t....   \n",
      "4  ['RT', '@NahBabyNah', ':', 'Twitchy', ':', 'Ch...   \n",
      "\n",
      "                                           stem_text  \\\n",
      "0     ['thingsdonebymistak', 'kiss', 'aunty', 'lip']   \n",
      "1  ['rt', 'mc_derpin', '', 'theolderweget', 'pess...   \n",
      "2  ['rt', 'dmatacon', '', 'ready', 'feel', 'lik',...   \n",
      "3                    ['am', '', 'blacklivesmat', '']   \n",
      "4  ['rt', 'nahbabynah', '', 'twitchy', '', 'chuck...   \n",
      "\n",
      "                                          lemma_text  diversity  \n",
      "0  ['thingsdonebymistake', 'kiss', 'auntie', 'lips']   0.448980  \n",
      "1  ['rt', 'mc_derpin', '', 'theolderweget', 'pess...   0.355932  \n",
      "2  ['rt', 'dmataconis', '', 'ready', 'feel', 'lik...   0.205357  \n",
      "3               ['amen', '', 'blacklivesmatter', '']   0.500000  \n",
      "4  ['rt', 'nahbabynah', '', 'twitchy', '', 'chuck...   0.153333  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matt/dev/ML/big-data-twitter/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, analyzer='word', ngram_range=(1, 2), stop_words='english')\n",
    "vz = vectorizer.fit_transform(list(data['tokens'].map(lambda tokens: ' '.join(tokens))))\n",
    "\n",
    "vz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
