{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMOD 5410 - Big Data\n",
    "## Project: Bot Detection\n",
    "### By: Matt Emmons (0221920)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Write stuff about this project. 3k+ words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.utils\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(101)\n",
    "rand_seed = 101\n",
    "\n",
    "n_rows = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.54 s, sys: 954 ms, total: 8.49 s\n",
      "Wall time: 8.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ROOT = \"./csv\"\n",
    "df_nbc = pd.read_csv(\n",
    "    ROOT + \"/tweets.csv\", \n",
    "    encoding='utf-8', \n",
    "    low_memory=False, \n",
    "    parse_dates=False\n",
    ")\n",
    "\n",
    "df_scraped = pd.read_csv(\n",
    "    ROOT + \"/scraped_tweets.csv\", \n",
    "    nrows=n_rows,\n",
    "#     converters = { 'text': lambda x: str(x.decode('utf-8')) },\n",
    "    encoding='utf-8', \n",
    "    low_memory=False, \n",
    "    parse_dates=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Let's remove unneeded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "delcols = [\n",
    "    'created_at',\n",
    "    'created_str',\n",
    "    'expanded_urls',\n",
    "    'in_reply_to_status_id',\n",
    "    'source'\n",
    "]\n",
    "for col in delcols:\n",
    "    del df_nbc[col]\n",
    "    del df_scraped[col]\n",
    "    \n",
    "del df_nbc['posted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to ensure all columns have consistent datatypes between the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nbc[['user_id', 'retweet_count', 'favorite_count', 'tweet_id', 'retweeted_status_id']] = df_nbc[['user_id', 'retweet_count', 'favorite_count', 'tweet_id', 'retweeted_status_id']].fillna(0).astype(int)\n",
    "df_nbc[['user_key', 'text']] = df_nbc[['user_key', 'text']].astype('str')\n",
    "df_nbc[['retweeted']] = df_nbc[['retweeted']].astype('bool')\n",
    "df_scraped[['retweeted_status_id']] = df_scraped[['retweeted_status_id']].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id                 int64\n",
      "user_key               object\n",
      "retweet_count           int64\n",
      "retweeted                bool\n",
      "favorite_count          int64\n",
      "text                   object\n",
      "tweet_id                int64\n",
      "hashtags               object\n",
      "mentions               object\n",
      "retweeted_status_id     int64\n",
      "dtype: object\n",
      "user_id                 int64\n",
      "user_key               object\n",
      "retweet_count           int64\n",
      "retweeted                bool\n",
      "favorite_count          int64\n",
      "text                   object\n",
      "tweet_id                int64\n",
      "hashtags               object\n",
      "mentions               object\n",
      "retweeted_status_id     int64\n",
      "dtype: object\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(df_nbc.dtypes)\n",
    "print(df_scraped.dtypes)\n",
    "\n",
    "print(list(df_nbc.dtypes) == list(df_scraped.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id         user_key  retweet_count  retweeted  favorite_count  \\\n",
      "0  2532611755        kathiemrr              0       True               0   \n",
      "1  2531159968   traceyhappymom              0       True               0   \n",
      "2           0    evewebster373              0       True               0   \n",
      "3  4840551713      blacktolive             18      False              17   \n",
      "4  1694026190  jacquelinisbest              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0    #ThingsDoneByMistake kissing auntie in the lips  836227891897651200   \n",
      "1  RT @mc_derpin: #TheOlderWeGet the more pessimi...  765198948239810560   \n",
      "2  RT @dmataconis: Ready To Feel Like A Failure? ...  616002306572746752   \n",
      "3    Amen! #blacklivesmatter https://t.co/wGffaOqgzl  776693302926147584   \n",
      "4  RT @NahBabyNah: Twitchy: Chuck Todd caught out...  777594647875059712   \n",
      "\n",
      "                  hashtags        mentions  retweeted_status_id  \n",
      "0  [\"ThingsDoneByMistake\"]              []                    0  \n",
      "1        [\"TheOlderWeGet\"]              []                    0  \n",
      "2                       []              []                    0  \n",
      "3     [\"Blacklivesmatter\"]              []                    0  \n",
      "4        [\"WakeUpAmerica\"]  [\"nahbabynah\"]   777591478206029824  \n",
      "\n",
      "---------------------\n",
      "\n",
      "              user_id         user_key  retweet_count  retweeted  \\\n",
      "0  961701459900342272  Jeannie22757716              0      False   \n",
      "1  961701477164105728        _p_body__              0      False   \n",
      "2  961701487507312641   6728FixerUpper              0      False   \n",
      "3  961701511289073665        LenAulett              0      False   \n",
      "4  961701912880988161       Dsquared69              0      False   \n",
      "\n",
      "   favorite_count                                               text  \\\n",
      "0               0  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...   \n",
      "1               0  b'\"We do not deserve any right \\xe2\\x80\\x94 an...   \n",
      "2               0  b'Just say no to Martha McSally, who voted for...   \n",
      "3               0  b'Pelosi Thanks Illegals for BREAKING THE LAW,...   \n",
      "4               0  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...   \n",
      "\n",
      "             tweet_id hashtags             mentions  retweeted_status_id  \n",
      "0  961701459900342272       []         ['bbusa617']   961624822953512960  \n",
      "1  961701477164105728       []      ['nprpolitics']   961390308553576448  \n",
      "2  961701487507312641       []  ['LarrySchweikart']   960538375546593280  \n",
      "3  961701511289073665       []         ['bbusa617']   961604790689165312  \n",
      "4  961701912880988161       []         ['bbusa617']   961624822953512960  \n"
     ]
    }
   ],
   "source": [
    "print(df_nbc.head())\n",
    "print(\"\\n---------------------\\n\")\n",
    "print(df_scraped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id         user_key  retweet_count  retweeted  favorite_count  \\\n",
      "0  2532611755        kathiemrr              0       True               0   \n",
      "1  2531159968   traceyhappymom              0       True               0   \n",
      "2           0    evewebster373              0       True               0   \n",
      "3  4840551713      blacktolive             18      False              17   \n",
      "4  1694026190  jacquelinisbest              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0    #ThingsDoneByMistake kissing auntie in the lips  836227891897651200   \n",
      "1  RT @mc_derpin: #TheOlderWeGet the more pessimi...  765198948239810560   \n",
      "2  RT @dmataconis: Ready To Feel Like A Failure? ...  616002306572746752   \n",
      "3    Amen! #blacklivesmatter https://t.co/wGffaOqgzl  776693302926147584   \n",
      "4  RT @NahBabyNah: Twitchy: Chuck Todd caught out...  777594647875059712   \n",
      "\n",
      "                  hashtags        mentions  retweeted_status_id  \n",
      "0  [\"ThingsDoneByMistake\"]              []                    0  \n",
      "1        [\"TheOlderWeGet\"]              []                    0  \n",
      "2                       []              []                    0  \n",
      "3     [\"Blacklivesmatter\"]              []                    0  \n",
      "4        [\"WakeUpAmerica\"]  [\"nahbabynah\"]   777591478206029824  \n",
      "\n",
      "---------------------\n",
      "\n",
      "              user_id         user_key  retweet_count  retweeted  \\\n",
      "0  961701459900342272  Jeannie22757716              0      False   \n",
      "1  961701477164105728        _p_body__              0      False   \n",
      "2  961701487507312641   6728FixerUpper              0      False   \n",
      "3  961701511289073665        LenAulett              0      False   \n",
      "4  961701912880988161       Dsquared69              0      False   \n",
      "\n",
      "   favorite_count                                               text  \\\n",
      "0               0  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...   \n",
      "1               0  b'\"We do not deserve any right \\xe2\\x80\\x94 an...   \n",
      "2               0  b'Just say no to Martha McSally, who voted for...   \n",
      "3               0  b'Pelosi Thanks Illegals for BREAKING THE LAW,...   \n",
      "4               0  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...   \n",
      "\n",
      "             tweet_id hashtags             mentions  retweeted_status_id  \n",
      "0  961701459900342272       []         ['bbusa617']   961624822953512960  \n",
      "1  961701477164105728       []      ['nprpolitics']   961390308553576448  \n",
      "2  961701487507312641       []  ['LarrySchweikart']   960538375546593280  \n",
      "3  961701511289073665       []         ['bbusa617']   961604790689165312  \n",
      "4  961701912880988161       []         ['bbusa617']   961624822953512960  \n"
     ]
    }
   ],
   "source": [
    "# df_nbc['newtext'] = df_nbc.text.str.decode(encoding = 'UTF-8')\n",
    "# df_scraped['newtext'] = df_scraped.text.str.decode(encoding = 'UTF-8')\n",
    "\n",
    "# df_nbc['newtext'] = df_nbc.text.apply( lambda x:  x.decode(encoding = \"utf-8\"))\n",
    "# df_scraped['newtext'] = df_scraped.text.apply( lambda x:  x.decode(encoding = \"utf-8\"))\n",
    "\n",
    "\n",
    "print(df_nbc.head())\n",
    "print(\"\\n---------------------\\n\")\n",
    "print(df_scraped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "\n",
    "What we need to do is this:\n",
    "\n",
    "- Attach a class feature to the nbc dataset\n",
    "- Since we do not know the class of the scraped dataset, we leave it for now\n",
    "- Create a new dataset merged between a subset of nbc and scraped datasets to be our training set\n",
    "- All rows left out of the merged subset will become the test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nbc['class'] = 1\n",
    "df_test = df_scraped.copy()\n",
    "df_test['class'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id         user_key  retweet_count  retweeted  favorite_count  \\\n",
      "0  2532611755        kathiemrr              0       True               0   \n",
      "1  2531159968   traceyhappymom              0       True               0   \n",
      "2           0    evewebster373              0       True               0   \n",
      "3  4840551713      blacktolive             18      False              17   \n",
      "4  1694026190  jacquelinisbest              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0    #ThingsDoneByMistake kissing auntie in the lips  836227891897651200   \n",
      "1  RT @mc_derpin: #TheOlderWeGet the more pessimi...  765198948239810560   \n",
      "2  RT @dmataconis: Ready To Feel Like A Failure? ...  616002306572746752   \n",
      "3    Amen! #blacklivesmatter https://t.co/wGffaOqgzl  776693302926147584   \n",
      "4  RT @NahBabyNah: Twitchy: Chuck Todd caught out...  777594647875059712   \n",
      "\n",
      "                  hashtags        mentions  retweeted_status_id  class  \n",
      "0  [\"ThingsDoneByMistake\"]              []                    0    1.0  \n",
      "1        [\"TheOlderWeGet\"]              []                    0    1.0  \n",
      "2                       []              []                    0    1.0  \n",
      "3     [\"Blacklivesmatter\"]              []                    0    1.0  \n",
      "4        [\"WakeUpAmerica\"]  [\"nahbabynah\"]   777591478206029824    1.0  \n",
      "user_id                  int64\n",
      "user_key                object\n",
      "retweet_count            int64\n",
      "retweeted                 bool\n",
      "favorite_count           int64\n",
      "text                    object\n",
      "tweet_id                 int64\n",
      "hashtags                object\n",
      "mentions                object\n",
      "retweeted_status_id      int64\n",
      "class                  float64\n",
      "dtype: object\n",
      "1203451\n"
     ]
    }
   ],
   "source": [
    "# Merge df_test and df_nbc\n",
    "# train/test/val split\n",
    "# apply classifiers\n",
    "df = pd.concat([df_nbc, df_test], ignore_index = True)\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "print(len(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: RT @mc_derpin: #TheOlderWeGet the more pessimistic we are https://t.co/zS3jHZJl8P\n",
      "Stems: ['rt', 'mc_derpin', 'theolderweget', 'pessim']\n",
      "Lemmas: ['rt', 'mc_derpin', 'theolderweget', 'pessimistic']\n",
      "Words: ['rt', 'mc_derpin', 'theolderweget', 'pessimistic']\n",
      "\n",
      "----\n",
      "\n",
      "Text: RT @mcicero10: #BernieSanders #Trump people should rally TOGETHER against the establishment who is ðŸ’©-ing on both choices #thefix\n",
      "Stems: ['rt', 'mcicero10', 'berniesand', 'trump', 'peopl', 'ral', 'togeth', 'est', 'ing', 'cho', 'thefix']\n",
      "Lemmas: ['rt', 'mcicero10', 'berniesanders', 'trump', 'people', 'rally', 'together', 'establishment', 'ing', 'choices', 'thefix']\n",
      "Words: ['rt', 'mcicero10', 'berniesanders', 'trump', 'people', 'rally', 'together', 'establishment', 'ing', 'choices', 'thefix']\n",
      "\n",
      "----\n",
      "\n",
      "Text: b'BREAKING: OBAMA DIRECTLY IMPLICATED IN FBI COVER-UP OF THE HILLARY INVES... https://t.co/ICxTmFEKIv via @YouTube'\n",
      "Stems: ['break', 'obam', 'direct', 'imply', 'fbi', 'coverup', 'hil', 'inv', 'via', 'youtub']\n",
      "Lemmas: ['break', 'obama', 'directly', 'implicate', 'fbi', 'coverup', 'hillary', 'inves', 'via', 'youtube']\n",
      "Words: ['breaking', 'obama', 'directly', 'implicated', 'fbi', 'coverup', 'hillary', 'inves', 'via', 'youtube']\n",
      "\n",
      "----\n",
      "\n",
      "Text: b'.@CharlesSchumer:\"Orthodox Jews Should Do More to Call out Trump for Failing to Condemn Neo-Nazis\"\\n\\nJews Should Do More to Call Out Schumer &amp;Antisemitic Democrat Party for Failing to Condemn Islamic Jew-Hatred, BDS &amp; Hatred of Israel.\\n\\nJews thank Gd for @realDonaldTrump \\n#MAGA'\n",
      "Stems: ['charlesschum', 'orthodox', 'jew', 'cal', 'trump', 'fail', 'condemn', 'neonaz', 'n', 'njew', 'cal', 'schumer', 'antisemit', 'democr', 'party', 'fail', 'condemn', 'islam', 'jewh', 'bds', 'hat', 'israel', 'n', 'njew', 'thank', 'gd', 'realdonaldtrump', 'n', 'mag']\n",
      "Lemmas: ['charlesschumer', 'orthodox', 'jews', 'call', 'trump', 'fail', 'condemn', 'neonazis', 'n', 'njews', 'call', 'schumer', 'antisemitic', 'democrat', 'party', 'fail', 'condemn', 'islamic', 'jewhatred', 'bds', 'hatred', 'israel', 'n', 'njews', 'thank', 'gd', 'realdonaldtrump', 'n', 'maga']\n",
      "Words: ['charlesschumer', 'orthodox', 'jews', 'call', 'trump', 'failing', 'condemn', 'neonazis', 'n', 'njews', 'call', 'schumer', 'antisemitic', 'democrat', 'party', 'failing', 'condemn', 'islamic', 'jewhatred', 'bds', 'hatred', 'israel', 'n', 'njews', 'thank', 'gd', 'realdonaldtrump', 'n', 'maga']\n",
      "\n",
      "----\n",
      "\n",
      "Text: b'CRAZZZZY TOWN !!\\n\\n1ST SCHIFF AND WARREN FOILED  \\xe2\\x80\\x94 NOW OBAMA\\xe2\\x80\\x99S FBI BUSTED TRYING TO PAY RUSSIANS 1 MILLION FOR DIRT ON TRUMP !!\\n\\n\\xe2\\x80\\x9cWhat is clear is the Obama Administration spied on a rival political campaign.\\xe2\\x80\\x9d \\xe2\\x80\\x94Tucker\\n\\nLOCK THEM UP !!! https://t.co/EQkZclNcbK'\n",
      "Stems: ['crazzzzy', 'town', 'n', 'n1st', 'schiff', 'war', 'foil', 'xe2', 'x80', 'x94', 'obam', 'xe2', 'x80', 'x99s', 'fbi', 'bust', 'try', 'pay', 'russ', 'on', 'mil', 'dirt', 'trump', 'n', 'n', 'xe2', 'x80', 'x9cwhat', 'clear', 'obam', 'admin', 'spi', 'riv', 'polit', 'campaign', 'xe2', 'x80', 'x9d', 'xe2', 'x80', 'x94tucker', 'n', 'nlock']\n",
      "Lemmas: ['crazzzzy', 'town', 'n', 'n1st', 'schiff', 'warren', 'foil', 'xe2', 'x80', 'x94', 'obama', 'xe2', 'x80', 'x99s', 'fbi', 'bust', 'try', 'pay', 'russians', 'one', 'million', 'dirt', 'trump', 'n', 'n', 'xe2', 'x80', 'x9cwhat', 'clear', 'obama', 'administration', 'spy', 'rival', 'political', 'campaign', 'xe2', 'x80', 'x9d', 'xe2', 'x80', 'x94tucker', 'n', 'nlock']\n",
      "Words: ['crazzzzy', 'town', 'n', 'n1st', 'schiff', 'warren', 'foiled', 'xe2', 'x80', 'x94', 'obama', 'xe2', 'x80', 'x99s', 'fbi', 'busted', 'trying', 'pay', 'russians', 'one', 'million', 'dirt', 'trump', 'n', 'n', 'xe2', 'x80', 'x9cwhat', 'clear', 'obama', 'administration', 'spied', 'rival', 'political', 'campaign', 'xe2', 'x80', 'x9d', 'xe2', 'x80', 'x94tucker', 'n', 'nlock']\n",
      "\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unicodedata, re\n",
    "\n",
    "class NormalizationPipeline():\n",
    "    \"\"\"\n",
    "    1. tokenize\n",
    "    2. replace_contractions\n",
    "    3. remove_non_ascii\n",
    "    4. to_lowercase\n",
    "    5. remove_punctuation\n",
    "    6. replace_numbers\n",
    "    7. remove_stopwords\n",
    "    \"\"\"\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        self.process()\n",
    "\n",
    "    def process(self):\n",
    "#         self.words = self.replace_contractions()\n",
    "#         self.words = nltk.word_tokenize(self.words)\n",
    "#         tt.tokenize\n",
    "        self.words = self.remove_non_ascii()\n",
    "        self.words = self.to_lowercase()\n",
    "        self.words = self.remove_b()\n",
    "        self.words = self.remove_links()\n",
    "        self.words = self.remove_punctuation()\n",
    "        self.words = self.replace_numbers()\n",
    "        self.words = self.remove_stopwords()\n",
    "        self.stems = self.stem_words()\n",
    "        self.lemmas = self.lemmatize_verbs()\n",
    "        return self.stems, self.lemmas, self.words\n",
    "\n",
    "    def replace_contractions(self):\n",
    "        \"\"\"Replace contractions in string of text\"\"\"\n",
    "        self.words = contractions.fix(self.words)\n",
    "        \n",
    "        \n",
    "    def remove_non_ascii(self):\n",
    "        \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in self.words:\n",
    "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def to_lowercase(self):\n",
    "        \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in self.words:\n",
    "            new_word = word.lower()\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def remove_b(self):\n",
    "        \"\"\"Remove the stupid byte string indicator\"\"\"\n",
    "        new_words = []\n",
    "        for word in self.words:\n",
    "            if word == self.words[0]:\n",
    "                if len(word) == 1 and word[0] == 'b':\n",
    "                    continue\n",
    "                elif len(word) > 1 and word[:2] == \"b'\":\n",
    "                    word = word[2:]\n",
    "                new_words.append(word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "    \n",
    "    def remove_links(self):\n",
    "        \"\"\"Remove links from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in self.words:\n",
    "            new_word = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', word)\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def remove_punctuation(self):\n",
    "        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in self.words:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def replace_numbers(self):\n",
    "        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "        p = inflect.engine()\n",
    "        new_words = []\n",
    "        for word in self.words:\n",
    "            if word.isdigit():\n",
    "                new_word = p.number_to_words(word)\n",
    "                new_words.append(new_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in self.words:\n",
    "            if word not in stopwords.words('english'):\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "\n",
    "    def stem_words(self):\n",
    "        \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "        stemmer = LancasterStemmer()\n",
    "        stems = []\n",
    "        for word in self.words:\n",
    "            stem = stemmer.stem(word)\n",
    "            stems.append(stem)\n",
    "        return stems\n",
    "\n",
    "    def lemmatize_verbs(self):\n",
    "        \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = []\n",
    "        for word in self.words:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "            lemmas.append(lemma)\n",
    "        return lemmas\n",
    "    \n",
    "# tt = TweetTokenizer()\n",
    "# df['tokenized_text'] = df['text'].apply(tt.tokenize) \n",
    "\n",
    "# count = 0\n",
    "# for idx, row in df.iterrows():\n",
    "#     print(row['tokenized_text'])\n",
    "#     count += 1\n",
    "#     if count == 10:\n",
    "#         break\n",
    "samples = [1, 5, 205100, 500192, 991858]\n",
    "\n",
    "for sample in samples:\n",
    "    tokenized = df.ix[sample].tokenized_text\n",
    "    text = df.ix[sample].text\n",
    "    stems, lemmas, words = NormalizationPipeline(tokenized).process()\n",
    "    print(\"Text: {}\".format(text))\n",
    "    print(\"Stems: {}\".format(stems))\n",
    "    print(\"Lemmas: {}\".format(lemmas))\n",
    "    print(\"Words: {}\".format(words))\n",
    "    print(\"\\n----\\n\")\n",
    "    \n",
    "# sample = df.ix[991858].tokenized_text\n",
    "# words = NormalizationPipeline(sample).process()\n",
    "# print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id         user_key  retweet_count  retweeted  favorite_count  \\\n",
      "0  2532611755        kathiemrr              0       True               0   \n",
      "1  2531159968   traceyhappymom              0       True               0   \n",
      "2           0    evewebster373              0       True               0   \n",
      "3  4840551713      blacktolive             18      False              17   \n",
      "4  1694026190  jacquelinisbest              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0    #ThingsDoneByMistake kissing auntie in the lips  836227891897651200   \n",
      "1  RT @mc_derpin: #TheOlderWeGet the more pessimi...  765198948239810560   \n",
      "2  RT @dmataconis: Ready To Feel Like A Failure? ...  616002306572746752   \n",
      "3    Amen! #blacklivesmatter https://t.co/wGffaOqgzl  776693302926147584   \n",
      "4  RT @NahBabyNah: Twitchy: Chuck Todd caught out...  777594647875059712   \n",
      "\n",
      "                  hashtags        mentions  retweeted_status_id  class  \\\n",
      "0  [\"ThingsDoneByMistake\"]              []                    0    1.0   \n",
      "1        [\"TheOlderWeGet\"]              []                    0    1.0   \n",
      "2                       []              []                    0    1.0   \n",
      "3     [\"Blacklivesmatter\"]              []                    0    1.0   \n",
      "4        [\"WakeUpAmerica\"]  [\"nahbabynah\"]   777591478206029824    1.0   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0  [#ThingsDoneByMistake, kissing, auntie, in, th...  \n",
      "1  [RT, @mc_derpin, :, #TheOlderWeGet, the, more,...  \n",
      "2  [RT, @dmataconis, :, Ready, To, Feel, Like, A,...  \n",
      "3  [Amen, !, #blacklivesmatter, https://t.co/wGff...  \n",
      "4  [RT, @NahBabyNah, :, Twitchy, :, Chuck, Todd, ...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Need to create features for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Diversity\n",
    "def lexical_diversity(text):\n",
    "    if len(text) == 0:\n",
    "        diversity = 0\n",
    "    else: \n",
    "        diversity = float(len(set(text))) / len(text)\n",
    "    return diversity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sklearn.utils.shuffle(df)\n",
    "X = df.iloc[:,0:7]\n",
    "Y = df.iloc[:,8]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'b\\'RT @mitchellvii: UNITE THE BASE! YourVoice\\\\xc2\\\\x99 America (2/8) \"Uranium One Connects Obama!\" https://t.co/c8ga2kr7Hy\\''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/dev/ML/big-data-twitter/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ML/big-data-twitter/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'b\\'RT @mitchellvii: UNITE THE BASE! YourVoice\\\\xc2\\\\x99 America (2/8) \"Uranium One Connects Obama!\" https://t.co/c8ga2kr7Hy\\''"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RFModel = RandomForestClassifier(\n",
    "    n_estimators = 1000, \n",
    "    max_depth = 5, \n",
    "    max_features = 3, \n",
    "    oob_score=False\n",
    ")\n",
    "\n",
    "RFModel.fit(X_train, Y_train)\n",
    "prediction = RFModel.predict_proba(X_test)\n",
    "auc = roc_auc_score(Y_test, prediction[:,1:2])\n",
    "print(auc)\n",
    "\n",
    "RFModel.fit(X_test, Y_test)\n",
    "prediction = RFModel.predict_proba(X_train)\n",
    "auc = roc_auc_score(Y_train, prediction[:,1:2])\n",
    "print(auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
