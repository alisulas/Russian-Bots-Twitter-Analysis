{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "np.random.seed(101)\n",
    "rand_seed = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ROOT = \"./csv\"\n",
    "df_nbc = pd.read_csv(\n",
    "    ROOT + \"/tweets.csv\", \n",
    "    encoding='utf-8', \n",
    "    low_memory=False, \n",
    "    parse_dates=False\n",
    ")\n",
    "\n",
    "df_scraped = pd.read_csv(\n",
    "    ROOT + \"/scraped_tweets.csv\", \n",
    "    encoding='utf-8',\n",
    "    nrows = 2000000,\n",
    "    low_memory=False, \n",
    "    parse_dates=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203451, 15)\n",
      "(2000000, 15)\n"
     ]
    }
   ],
   "source": [
    "del df_nbc['posted']\n",
    "\n",
    "print(df_nbc.shape)\n",
    "print(df_scraped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id                   int64\n",
      "user_key                 object\n",
      "created_at                int64\n",
      "created_str              object\n",
      "retweet_count             int64\n",
      "retweeted                  bool\n",
      "favorite_count            int64\n",
      "text                     object\n",
      "tweet_id                  int64\n",
      "source                   object\n",
      "hashtags                 object\n",
      "expanded_urls            object\n",
      "mentions                 object\n",
      "retweeted_status_id       int64\n",
      "in_reply_to_status_id     int64\n",
      "dtype: object\n",
      "user_id                   int64\n",
      "user_key                 object\n",
      "created_at                int64\n",
      "created_str              object\n",
      "retweet_count             int64\n",
      "retweeted                  bool\n",
      "favorite_count            int64\n",
      "text                     object\n",
      "tweet_id                  int64\n",
      "source                   object\n",
      "hashtags                 object\n",
      "expanded_urls            object\n",
      "mentions                 object\n",
      "retweeted_status_id       int64\n",
      "in_reply_to_status_id     int64\n",
      "dtype: object\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Ensure all columns have the same datatypes\n",
    "df_nbc[[\n",
    "    'user_id', \n",
    "    'retweet_count', \n",
    "    'favorite_count', \n",
    "    'tweet_id', \n",
    "    'retweeted_status_id',\n",
    "    'created_at',\n",
    "    'in_reply_to_status_id'\n",
    "]] = df_nbc[[\n",
    "    'user_id', \n",
    "    'retweet_count', \n",
    "    'favorite_count', \n",
    "    'tweet_id', \n",
    "    'retweeted_status_id',\n",
    "    'created_at',\n",
    "    'in_reply_to_status_id'\n",
    "]].fillna(0).astype(np.int64)\n",
    "\n",
    "df_nbc[['user_key', 'text']] = df_nbc[['user_key', 'text']].astype('str')\n",
    "df_nbc[['retweeted']] = df_nbc[['retweeted']].astype('bool')\n",
    "\n",
    "df_scraped[[\n",
    "    'retweeted_status_id',\n",
    "    'in_reply_to_status_id'\n",
    "]] = df_scraped[[\n",
    "    'retweeted_status_id',\n",
    "    'in_reply_to_status_id'\n",
    "]].fillna(0).astype(np.int64)\n",
    "\n",
    "print(df_nbc.dtypes)\n",
    "print(df_scraped.dtypes)\n",
    "\n",
    "print(list(df_nbc.dtypes) == list(df_scraped.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id         user_key     created_at          created_str  \\\n",
      "0  2532611755        kathiemrr  1488207240000  2017-02-27 14:54:00   \n",
      "1  2531159968   traceyhappymom  1471272620000  2016-08-15 14:50:20   \n",
      "2           0    evewebster373  1435701369000  2015-06-30 21:56:09   \n",
      "3  4840551713      blacktolive  1474013088000  2016-09-16 08:04:48   \n",
      "4  1694026190  jacquelinisbest  1474227985000  2016-09-18 19:46:25   \n",
      "\n",
      "   retweet_count  retweeted  favorite_count  \\\n",
      "0              0       True               0   \n",
      "1              0       True               0   \n",
      "2              0       True               0   \n",
      "3             18      False              17   \n",
      "4              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0    #ThingsDoneByMistake kissing auntie in the lips  836227891897651200   \n",
      "1  RT @mc_derpin: #TheOlderWeGet the more pessimi...  765198948239810560   \n",
      "2  RT @dmataconis: Ready To Feel Like A Failure? ...  616002306572746752   \n",
      "3    Amen! #blacklivesmatter https://t.co/wGffaOqgzl  776693302926147584   \n",
      "4  RT @NahBabyNah: Twitchy: Chuck Todd caught out...  777594647875059712   \n",
      "\n",
      "                                              source                 hashtags  \\\n",
      "0                                                NaN  [\"ThingsDoneByMistake\"]   \n",
      "1                                                NaN        [\"TheOlderWeGet\"]   \n",
      "2                                                NaN                       []   \n",
      "3  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...     [\"Blacklivesmatter\"]   \n",
      "4  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...        [\"WakeUpAmerica\"]   \n",
      "\n",
      "                                 expanded_urls        mentions  \\\n",
      "0                                           []              []   \n",
      "1                                           []              []   \n",
      "2                                           []              []   \n",
      "3                                           []              []   \n",
      "4  [\"http://ln.is/twitchy.com/loriz-31/3yafU\"]  [\"nahbabynah\"]   \n",
      "\n",
      "   retweeted_status_id  in_reply_to_status_id  \n",
      "0                    0                      0  \n",
      "1                    0                      0  \n",
      "2                    0                      0  \n",
      "3                    0                      0  \n",
      "4   777591478206029824                      0  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "              user_id         user_key     created_at  \\\n",
      "0  961701459900342272  Jeannie22757716  1518122470246   \n",
      "1  961701477164105728        _p_body__  1518122474362   \n",
      "2  961701487507312641   6728FixerUpper  1518122476828   \n",
      "3  961701511289073665        LenAulett  1518122482498   \n",
      "4  961701912880988161       Dsquared69  1518122578245   \n",
      "\n",
      "                      created_str  retweet_count  retweeted  favorite_count  \\\n",
      "0  Thu Feb 08 20:41:10 +0000 2018              0      False               0   \n",
      "1  Thu Feb 08 20:41:14 +0000 2018              0      False               0   \n",
      "2  Thu Feb 08 20:41:16 +0000 2018              0      False               0   \n",
      "3  Thu Feb 08 20:41:22 +0000 2018              0      False               0   \n",
      "4  Thu Feb 08 20:42:58 +0000 2018              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...  961701459900342272   \n",
      "1  b'\"We do not deserve any right \\xe2\\x80\\x94 an...  961701477164105728   \n",
      "2  b'Just say no to Martha McSally, who voted for...  961701487507312641   \n",
      "3  b'Pelosi Thanks Illegals for BREAKING THE LAW,...  961701511289073665   \n",
      "4  b'JUST IN: GEORGE W. BUSH In Abu Dhabi\\xe2\\x80...  961701912880988161   \n",
      "\n",
      "                                              source hashtags  \\\n",
      "0  <a href=\"http://twitter.com/download/android\" ...       []   \n",
      "1  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       []   \n",
      "2  <a href=\"http://twitter.com/download/android\" ...       []   \n",
      "3  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...       []   \n",
      "4  <a href=\"http://twitter.com/download/iphone\" r...       []   \n",
      "\n",
      "                                       expanded_urls             mentions  \\\n",
      "0                                                 []         ['bbusa617']   \n",
      "1                                                 []      ['nprpolitics']   \n",
      "2                                                 []  ['LarrySchweikart']   \n",
      "3  ['http://truthfeednews.com/pelosi-thanks-illeg...         ['bbusa617']   \n",
      "4                                                 []         ['bbusa617']   \n",
      "\n",
      "   retweeted_status_id  in_reply_to_status_id  \n",
      "0   961624822953512960                      0  \n",
      "1   961390308553576448                      0  \n",
      "2   960538375546593280                      0  \n",
      "3   961604790689165312                      0  \n",
      "4   961624822953512960                      0  \n"
     ]
    }
   ],
   "source": [
    "print(df_nbc.head())\n",
    "print(\"\\n\\n\\n\")\n",
    "print(df_scraped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "\n",
    "What we need to do is this:\n",
    "\n",
    "- Attach a class feature to the nbc dataset\n",
    "- Since we do not know the class of the scraped dataset, we leave it for now\n",
    "- Create a new dataset merged between a subset of nbc and scraped datasets to be our training set\n",
    "- All rows left out of the merged subset will become the test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id         user_key     created_at          created_str  \\\n",
      "0  2532611755        kathiemrr  1488207240000  2017-02-27 14:54:00   \n",
      "1  2531159968   traceyhappymom  1471272620000  2016-08-15 14:50:20   \n",
      "2           0    evewebster373  1435701369000  2015-06-30 21:56:09   \n",
      "3  4840551713      blacktolive  1474013088000  2016-09-16 08:04:48   \n",
      "4  1694026190  jacquelinisbest  1474227985000  2016-09-18 19:46:25   \n",
      "\n",
      "   retweet_count  retweeted  favorite_count  \\\n",
      "0              0       True               0   \n",
      "1              0       True               0   \n",
      "2              0       True               0   \n",
      "3             18      False              17   \n",
      "4              0      False               0   \n",
      "\n",
      "                                                text            tweet_id  \\\n",
      "0    #ThingsDoneByMistake kissing auntie in the lips  836227891897651200   \n",
      "1  RT @mc_derpin: #TheOlderWeGet the more pessimi...  765198948239810560   \n",
      "2  RT @dmataconis: Ready To Feel Like A Failure? ...  616002306572746752   \n",
      "3    Amen! #blacklivesmatter https://t.co/wGffaOqgzl  776693302926147584   \n",
      "4  RT @NahBabyNah: Twitchy: Chuck Todd caught out...  777594647875059712   \n",
      "\n",
      "                                              source                 hashtags  \\\n",
      "0                                                NaN  [\"ThingsDoneByMistake\"]   \n",
      "1                                                NaN        [\"TheOlderWeGet\"]   \n",
      "2                                                NaN                       []   \n",
      "3  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...     [\"Blacklivesmatter\"]   \n",
      "4  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...        [\"WakeUpAmerica\"]   \n",
      "\n",
      "                                 expanded_urls        mentions  \\\n",
      "0                                           []              []   \n",
      "1                                           []              []   \n",
      "2                                           []              []   \n",
      "3                                           []              []   \n",
      "4  [\"http://ln.is/twitchy.com/loriz-31/3yafU\"]  [\"nahbabynah\"]   \n",
      "\n",
      "   retweeted_status_id  in_reply_to_status_id  class  \n",
      "0                    0                      0    1.0  \n",
      "1                    0                      0    1.0  \n",
      "2                    0                      0    1.0  \n",
      "3                    0                      0    1.0  \n",
      "4   777591478206029824                      0    1.0  \n",
      "user_id                    int64\n",
      "user_key                  object\n",
      "created_at                 int64\n",
      "created_str               object\n",
      "retweet_count              int64\n",
      "retweeted                   bool\n",
      "favorite_count             int64\n",
      "text                      object\n",
      "tweet_id                   int64\n",
      "source                    object\n",
      "hashtags                  object\n",
      "expanded_urls             object\n",
      "mentions                  object\n",
      "retweeted_status_id        int64\n",
      "in_reply_to_status_id      int64\n",
      "class                    float64\n",
      "dtype: object\n",
      "2203451\n"
     ]
    }
   ],
   "source": [
    "df_nbc['class'] = 1\n",
    "df_test = df_scraped.copy()\n",
    "df_test['class'] = np.nan\n",
    "\n",
    "# Merge df_test and df_nbc\n",
    "# train/test/val split\n",
    "# apply classifiers\n",
    "df = pd.concat([df_nbc, df_test], ignore_index = True)\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "print(len(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iterate through each row\n",
    "- Tokenize text\n",
    "- Normalize text\n",
    "- Stem/Lemma text\n",
    "- Save entire row to new spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "headers = list(df.columns.values)\n",
    "headers.extend(['tokenized_text', 'stem_text', 'lemma_text'])\n",
    "with open(\".\\\\csv\\\\mergedtweets.csv\", 'w', encoding = 'utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames = headers, lineterminator = '\\n')\n",
    "        writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tt = TweetTokenizer()\n",
    "\n",
    "def preprocess_text(tokenized_text):\n",
    "    \"\"\"\n",
    "    Accepts an array of strings\n",
    "    Handles all the preprocessing of the tokenized text before stemming/lemma\n",
    "    - removes 'b'\n",
    "    - removes unicode symbols\n",
    "    - replaces contractions\n",
    "    - removes non-ascii symbols\n",
    "    - converts to lowercase\n",
    "    - removes punctuation\n",
    "    - removes numbers\n",
    "    \"\"\"\n",
    "    new_words = []\n",
    "    for word in tokenized_text:\n",
    "        # remove 'b' from tokens\n",
    "        if word == tokenized_text[0]:\n",
    "            if len(word) == 1 and word[0] == 'b':\n",
    "                continue\n",
    "            elif len(word) > 1 and word[:2] == \"b'\":\n",
    "                word = word[2:]\n",
    "                \n",
    "        # remove unicode symbols from tokens\n",
    "        word = re.sub(r\"(x[abcdef0-9]{0,2})?\", '', word)\n",
    "        # word = remove_unicode(word)\n",
    "        \n",
    "        # replace contractions from tokens\n",
    "        word = contractions.fix(word)\n",
    "        # word = replace_contractions(word)\n",
    "        \n",
    "        # remove non-ascii from tokens\n",
    "        word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        # word = remove_non_ascii(word)\n",
    "        \n",
    "        # convert word to lowercase\n",
    "        word = word.lower()\n",
    "        \n",
    "        # remove links from token\n",
    "        word = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', word)\n",
    "        # word = remove_links(word)\n",
    "        \n",
    "        # remove punctuation if it results in an actual world\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '' or new_word != None:\n",
    "            word = new_word\n",
    "            \n",
    "        # replace numbers with textual representation\n",
    "        word = replace_numbers(word)\n",
    "        \n",
    "        # skip over errant 'n'\n",
    "        if len(word) == 1 and word[0] == 'n':\n",
    "            continue\n",
    "            \n",
    "        # skip over all stopwords\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_unicode(word):\n",
    "    new_word = re.sub(r\"(x[abcdef0-9]{0,2})?\", '', word)\n",
    "    return new_word\n",
    "\n",
    "def replace_contractions(word):\n",
    "    new_word = contractions.fix(word)\n",
    "    return new_word\n",
    "\n",
    "def remove_non_ascii(word):\n",
    "    new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_word\n",
    "\n",
    "def remove_links(word):\n",
    "    new_word = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', word)\n",
    "    return new_word\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    if new_word != '':\n",
    "        return new_word\n",
    "    \n",
    "def replace_numbers(word):\n",
    "    if word.isdigit():\n",
    "        p = inflect.engine()\n",
    "        new_word = p.number_to_words(word)\n",
    "        return new_word\n",
    "        new_words.append(new_word)\n",
    "    else:\n",
    "        return word \n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def handle_text(row):\n",
    "    \"\"\"\n",
    "    Tokenize, Stem and Lemma the text.\n",
    "    Append these values, in that order, to row.values\n",
    "    \"\"\"\n",
    "    text = row['text']\n",
    "    tokenized = tt.tokenize(text)\n",
    "    processed = preprocess_text(tokenized)\n",
    "    stems = stem_words(processed)\n",
    "    lemmas = lemmatize_words(processed)\n",
    "#     print(\"\\nTokenized: {}\".format(tokenized))\n",
    "#     print(\"Stems: {}\".format(stems))\n",
    "#     print(\"Lemmas: {}\\n\".format(lemmas))\n",
    "    return [tokenized, stems, lemmas]\n",
    "    \n",
    "def write_to_csv(row):\n",
    "    with open(\".\\\\csv\\\\mergedtweets.csv\", 'a', encoding = 'utf-8', newline = '') as file:\n",
    "        writer = csv.writer(file, delimiter = ',')\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a777ff74584d1e9b97e9dbe839c0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# samples = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 518, 5901, 9102, 300005, 401509, 567891, 991858, 991500, 991918]\n",
    "\n",
    "def analyze():\n",
    "    with open(\".\\\\csv\\\\mergedtweets.csv\", 'a', encoding = 'utf-8', newline = '') as file:\n",
    "        writer = csv.writer(file, delimiter = ',')\n",
    "        for index, row in tqdm(df.iterrows()):\n",
    "            augmented = handle_text(row)\n",
    "            values = list(row.values)\n",
    "            values.extend(augmented)\n",
    "            writer.writerow(values)\n",
    "\n",
    "analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
